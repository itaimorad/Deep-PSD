{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bY4u2pfIwWwOXF1CkANT10elzpIw1zEz","timestamp":1694956563525}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch.optim.lr_scheduler as lr_scheduler"],"metadata":{"id":"QAISg0Xpq2Ay"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Charge Integration"],"metadata":{"id":"s3Co9eKQjusd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"iwwL8kqujm5d"},"outputs":[],"source":["def charge_integration(discrete_data,CI_parameters, starting_index = 0):\n","  sg = int(CI_parameters[0])\n","  lg = int(CI_parameters[1])\n","  st_pt = int(CI_parameters[2])\n","  THR = CI_parameters[3]\n","\n","  ql = np.sum(discrete_data[:, st_pt:st_pt + lg], axis=1)\n","  qs = np.sum(discrete_data[:, st_pt:st_pt + sg], axis=1)\n","  CI_values = (ql - qs) / ql\n","  CI_labels = np.where(CI_values > THR, 0, 1)\n","\n","  return CI_labels, CI_values\n","\n","def optimize_charge_integration_parameters(discrete_data,discrete_labels, starting_index = 0):\n","  best_accuracy = 0\n","  best_sg = 0\n","  best_lg = 0\n","  best_PSD_THR = 0\n","  best_ci_st_pt = 0\n","\n","  for sg in CI_param1_lst:\n","    for lg in CI_param2_lst:\n","      for st_pt in CI_param3_lst:\n","        if sg >= lg:\n","          break\n","\n","        ql = np.sum(discrete_data[:, st_pt:st_pt + lg], axis=1)\n","        qs = np.sum(discrete_data[:, st_pt:st_pt + sg], axis=1)\n","        psd_ratio = (ql - qs) / ql\n","\n","        for THR in CI_param4_lst:\n","          correct = np.sum(np.where(psd_ratio > THR, 0, 1) == discrete_labels)\n","          accuracy = 100*correct/len(discrete_labels)\n","          if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","            best_sg = sg\n","            best_lg = lg\n","            best_PSD_THR = THR\n","            best_ci_st_pt = st_pt\n","\n","  return [best_sg,best_lg,best_ci_st_pt,best_PSD_THR]"]},{"cell_type":"markdown","source":["# Correlation"],"metadata":{"id":"fBp3A_sejyEX"}},{"cell_type":"code","source":["def correlation_classifier(discrete_data,correlation_parameters, starting_index = 0):\n","  st_pt = int(correlation_parameters[0])\n","  temp_end = int(correlation_parameters[1])\n","\n","  curr_temp_n = np.copy(template_n)\n","  curr_temp_g = np.copy(template_g)\n","  curr_temp_n = curr_temp_n[st_pt:temp_end]\n","  curr_temp_g = curr_temp_g[st_pt:temp_end]\n","  curr_data = discrete_data[:,time_offset + st_pt:time_offset + temp_end]\n","\n","  mean_n = np.mean(curr_temp_n)\n","  sigma_n = np.std(curr_temp_n)\n","  mean_g = np.mean(curr_temp_g)\n","  sigma_g = np.std(curr_temp_g)\n","  mean_x = np.mean(curr_data, axis=1)\n","  sigma_x = np.std(curr_data, axis=1)\n","  corr_n = np.dot(curr_data - mean_x[:, np.newaxis], curr_temp_n - mean_n) / (temp_end * sigma_x * sigma_n)\n","  corr_g = np.dot(curr_data - mean_x[:, np.newaxis], curr_temp_g - mean_g) / (temp_end * sigma_x * sigma_g)\n","\n","  corr_labels = np.where(corr_n > corr_g, 0, 1)\n","  corr_values = corr_n - corr_g\n","\n","  return corr_labels, corr_values\n","\n","def optimize_correlation_parameters(discrete_data,discrete_labels, starting_index = 0):\n","\n","  best_accuracy = 0\n","  best_temp_end = 0\n","  best_st_pt = 0\n","\n","  for st_pt in corr_param1_lst:\n","    for temp_end in corr_param2_lst:\n","\n","      curr_temp_n = np.copy(template_n)\n","      curr_temp_g = np.copy(template_g)\n","      curr_temp_n = curr_temp_n[st_pt:temp_end]\n","      curr_temp_g = curr_temp_g[st_pt:temp_end]\n","      curr_data = discrete_data[:,time_offset + st_pt: time_offset + temp_end]\n","\n","      mean_n = np.mean(curr_temp_n)\n","      sigma_n = np.std(curr_temp_n)\n","      mean_g = np.mean(curr_temp_g)\n","      sigma_g = np.std(curr_temp_g)\n","      mean_x = np.mean(curr_data, axis=1)\n","      sigma_x = np.std(curr_data, axis=1)\n","\n","      corr_n = np.dot(curr_data - mean_x[:, np.newaxis], curr_temp_n - mean_n) / (temp_end * sigma_x * sigma_n)\n","      corr_g = np.dot(curr_data - mean_x[:, np.newaxis], curr_temp_g - mean_g) / (temp_end * sigma_x * sigma_g)\n","\n","      correct = np.sum(np.where(corr_n > corr_g, 0, 1) == discrete_labels)\n","      accuracy = 100*correct/len(discrete_labels)\n","      if accuracy > best_accuracy:\n","        best_accuracy = accuracy\n","        best_temp_end = temp_end\n","        best_st_pt = st_pt\n","\n","  return [best_st_pt,best_temp_end]"],"metadata":{"id":"W7ZxDwbGjzF6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fully Connected NN (DNN applied to x_t)"],"metadata":{"id":"OVjoqcbUlqrk"}},{"cell_type":"code","source":["class Model_FC_NN(nn.Module):\n","  def __init__(self,dim=WL_classifier):\n","    super().__init__()\n","    self.fct0 = nn.Linear(dim,model_classifier_layer1)\n","    self.fct = nn.Linear(model_classifier_layer1,model_classifier_layer2)\n","    self.fc1 = nn.Linear(model_classifier_layer2,model_classifier_layer3)\n","    self.fc2 = nn.Linear(model_classifier_layer3,model_classifier_layer4)\n","    self.fc3 = nn.Linear(model_classifier_layer4,model_classifier_layer5)\n","    self.fc4 = nn.Linear(model_classifier_layer5,1)\n","\n","  def forward(self,x):\n","    x = self.fct0(x)\n","    x = F.relu(x)\n","    x = self.fct(x)\n","    x = F.relu(x)\n","    x = self.fc1(x)\n","    x = F.relu(x)\n","    x = self.fc2(x)\n","    x = F.relu(x)\n","    x = self.fc3(x)\n","    x = F.relu(x)\n","    x = self.fc4(x)\n","    x = F.sigmoid(x)\n","\n","    return x\n","\n","\n","def train_Model_FC_NN(model, train_data, train_labels, val_data, val_labels, learning_rate, batch_size, epochs, patience=20, factor=0.2):\n","\n","    flag_start_again = 0\n","    while(flag_start_again != 2):\n","      flag_start_again = 0\n","      optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","      criterion = nn.BCELoss()\n","      scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience, factor=factor, verbose=True)\n","\n","      losses, epoch_lst, train_accs, val_accs = [], [], [], []\n","      length_train = np.shape(train_data)[0]\n","      length_val = np.shape(val_data)[0]\n","\n","      best_val_acc = 0\n","      tj = 0\n","      flagy = 0\n","      for j in range(epochs):\n","          if flagy == 1 or flag_start_again == 1:\n","            break\n","          perm = np.random.permutation(length_train)\n","          train_data = train_data[perm]\n","          train_labels = train_labels[perm]\n","\n","          for i in range(0, length_train, batch_size):\n","              if (i + batch_size) > length_train:\n","                  break\n","              y_in = train_data[i:i+batch_size, :]\n","              labels_in = train_labels[i:i+batch_size]\n","              y_in = torch.Tensor(y_in)\n","              labels_in = torch.Tensor(labels_in).float().unsqueeze(1)\n","\n","              output = model(y_in)\n","              loss = criterion(output, labels_in)\n","              optimizer.zero_grad()\n","              loss.backward()\n","              optimizer.step()\n","\n","              if i % int(batch_size*300) == 0:\n","                losses.append(float(loss)/batch_size)  # compute *average* loss\n","                epoch_lst.append(tj)\n","                train_cost = float(loss.detach().numpy())\n","                train_acc = estimate_accuracy_Model_FC_NN(model, train_data, train_labels)\n","                train_accs.append(train_acc)\n","                val_acc = estimate_accuracy_Model_FC_NN(model, val_data, val_labels)\n","                val_accs.append(val_acc)\n","                print(\"Epoch %d. [Val Acc %.2f%%] [Train Acc %.2f%%, Loss %f]\" % (\n","                      j, val_acc, train_acc, train_cost))\n","                tj += 1\n","\n","                scheduler.step(val_acc)\n","                for param_group in optimizer.param_groups:\n","                    curr_lr = param_group['lr']\n","                if curr_lr == learning_rate*np.power(factor,3):\n","                    print(\"Training stopped. No improvement in validation accuracy for %d epochs.\" % patience)\n","                    flagy = 1\n","                    flag_start_again = 2\n","                    break\n","\n","                if tj == 10 and val_acc < 53:\n","                  flag_start_again = 1\n","\n","    del y_in\n","    del labels_in\n","    return losses, epoch_lst, train_accs, val_accs\n","\n","def estimate_accuracy_Model_FC_NN(model, data, labels):\n","\n","    data = torch.Tensor(data)\n","    labels = torch.Tensor(labels).float().unsqueeze(1)\n","    out = model(data)\n","    correct_mask = torch.round(out) == labels\n","    correct = correct_mask.sum().item()\n","    all = correct_mask.numel()\n","\n","    return 100 * correct / all\n","\n","def create_labels_Model_FC_NN(model,data,params):\n","\n","  data = (data - params[0])/params[1]\n","  data = torch.Tensor(data)\n","  nn_values = model(data)\n","  labels_out = torch.round(nn_values)\n","\n","  return np.squeeze(labels_out.detach().numpy()), np.squeeze(nn_values.detach().numpy())\n","\n","\n","def run_Model_FC_NN(labels_train,data_train,labels_val,data_val,labels_test,data_test,path1, path2, learning_rate=1e-3, batch_size=50, epochs=100):\n","  model = Model_FC_NN()\n","\n","  mu, sigma = find_normal_parameters(data_train)\n","  norm_train,norm_val,norm_test = normalize_datasets(data_train,data_val,data_test,mu,sigma)\n","\n","  learning_curve_info = train_Model_FC_NN(model,norm_train\n","                                                   , labels_train, norm_val\n","                                                   , labels_val, learning_rate, batch_size, epochs)\n","\n","  plot_learning_curve(*learning_curve_info)\n","\n","  test_accuracy = estimate_accuracy_Model_FC_NN(model, norm_test, labels_test)\n","  print(\"[Test Acc %.2f%%]\" % (test_accuracy))\n","\n","  torch.save(model, path1)\n","  np.save(path2,np.array([mu,sigma,WL_classifier,learning_rate,batch_size,epochs]))\n","\n","  return"],"metadata":{"id":"52iYa-k8Choi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Mode 1: DNN applied to x_t + y_t. Mode 2: Deep Classifier"],"metadata":{"id":"qsAoUr67lteZ"}},{"cell_type":"code","source":["class Model_Attention(nn.Module):\n","  def __init__(self,y_dim = WL_classifier, z_dim = num_classification_features, m_dim = num_attention_features):\n","    super().__init__()\n","    self.fc1 = nn.Linear((y_dim+z_dim), model_classifier_layer1)\n","    self.fc2 = nn.Linear(model_classifier_layer1,model_classifier_layer2)\n","    self.fc3 = nn.Linear(model_classifier_layer2,model_classifier_layer3)\n","    self.fc4 = nn.Linear(model_classifier_layer3,model_classifier_layer4)\n","    self.fc5 = nn.Linear(model_classifier_layer4,model_classifier_layer5)\n","    self.fc6 = nn.Linear(model_classifier_layer5,1)\n","\n","    self.r_fc1 = nn.Linear(m_dim, model_attention_layer1)\n","    self.r_fc2 = nn.Linear(model_attention_layer1, model_attention_layer2)\n","    self.r_fc3 = nn.Linear(model_attention_layer2, (y_dim+z_dim))\n","\n","  def forward(self,y,z,m,mode):\n","    size1 = np.shape(y)[0]\n","    size2 = np.shape(z)[0]\n","    size3 = np.shape(m)[0]\n","\n","    if size1 != size2:\n","      x = tf.concat([y,z],axis=0)\n","    else:\n","      x = tf.concat([y,z],axis=1)\n","\n","    if mode == 1:\n","      x = x.numpy()\n","      x = torch.Tensor(x)\n","      x = self.fc1(x)\n","      x = F.relu(x)\n","      x = self.fc2(x)\n","      x = F.relu(x)\n","      x = self.fc3(x)\n","      x = F.relu(x)\n","      x = self.fc4(x)\n","      x = F.relu(x)\n","      x = self.fc5(x)\n","      x = F.relu(x)\n","      x = self.fc6(x)\n","      x = F.sigmoid(x)\n","\n","      return x\n","\n","    if mode == 2:\n","\n","      for param in self.fc1.parameters():\n","          param.requires_grad = False\n","      for param in self.fc2.parameters():\n","          param.requires_grad = False\n","      for param in self.fc3.parameters():\n","          param.requires_grad = False\n","      for param in self.fc4.parameters():\n","          param.requires_grad = False\n","      for param in self.fc5.parameters():\n","          param.requires_grad = False\n","      for param in self.fc6.parameters():\n","          param.requires_grad = False\n","\n","      r = self.r_fc1(m)\n","      r = F.relu(r)\n","      r = self.r_fc2(r)\n","      r = F.relu(r)\n","      r = self.r_fc3(r)\n","      r = F.sigmoid(r)\n","\n","      x = x.numpy()\n","      x = torch.Tensor(x)\n","\n","      x = r*x\n","\n","      x = self.fc1(x)\n","      x = F.relu(x)\n","      x = self.fc2(x)\n","      x = F.relu(x)\n","      x = self.fc3(x)\n","      x = F.relu(x)\n","      x = self.fc4(x)\n","      x = F.relu(x)\n","      x = self.fc5(x)\n","      x = F.relu(x)\n","      x = self.fc6(x)\n","      x = F.sigmoid(x)\n","\n","      return x\n","\n","\n","def train_Model_Attention(model,train_data,train_labels,val_data,val_labels,\n","                          train_data_class,val_data_class,train_data_weigh,val_data_weigh,learning_rate,batch_size,epochs, path1, patience=20, factor=0.2):\n","\n","  flag_start_again = 0\n","  while(flag_start_again != 2):\n","    flag_start_again = 0\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    criterion = nn.BCELoss()\n","\n","    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience, factor=factor, verbose=True)\n","\n","    losses, epoch_lst, train_accs, val_accs  = [], [] ,[], []\n","    length_train = np.shape(train_data)[0]\n","    length_val = np.shape(val_data)[0]\n","\n","    best_val_acc = 0\n","    tj = 0\n","    mode = 1\n","    flag_mode_2 = 0\n","    flag_mode_3 = 0\n","    for j in range(epochs):\n","\n","        if flag_start_again == 1:\n","          break\n","\n","        if mode == 2 and flag_mode_2 == 0:\n","          flag_mode_2 = 1\n","          print(\"Stage 2 starts now\")\n","          optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","          scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience, factor=factor, verbose=True)\n","\n","        if mode == 3:\n","          break\n","\n","        perm = np.random.permutation(length_train)\n","        train_data = train_data[perm]\n","        train_labels = train_labels[perm]\n","        train_data_class = train_data_class[perm]\n","        train_data_weigh = train_data_weigh[perm]\n","\n","        for i in range(0, length_train, batch_size):\n","            if (i + batch_size) > np.shape(train_data)[0]:\n","                break\n","            x_in = train_data[i:i+batch_size,:]\n","            y_in = train_data_class[i:i+batch_size,:]\n","            z_in = train_data_weigh[i:i+batch_size,:]\n","            x_in = torch.Tensor(x_in)\n","            y_in = torch.Tensor(y_in)\n","            z_in = torch.Tensor(z_in)\n","            labels_in = train_labels[i:i+batch_size]\n","            labels_in = torch.Tensor(labels_in).float().unsqueeze(1)\n","\n","            output = model(x_in,y_in,z_in,mode)\n","            loss = criterion(output, labels_in)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            if i % int(batch_size*300) == 0:\n","              losses.append(float(loss)/batch_size)  # compute *average* loss\n","              epoch_lst.append(tj)\n","              train_cost = float(loss.detach().numpy())\n","              train_acc = estimate_accuracy_Model_Attention(model, train_data, train_labels,train_data_class,train_data_weigh,curr_mode=mode)\n","              train_accs.append(train_acc)\n","              val_acc = estimate_accuracy_Model_Attention(model, val_data, val_labels,val_data_class,val_data_weigh,curr_mode=mode)\n","              val_accs.append(val_acc)\n","              print(\"Epoch %d. [Val Acc %.2f%%] [Train Acc %.2f%%, Loss %f]\" % (\n","                    j, val_acc, train_acc, train_cost))\n","              tj += 1\n","\n","              scheduler.step(val_acc)\n","              for param_group in optimizer.param_groups:\n","                  curr_lr = param_group['lr']\n","              if curr_lr == learning_rate*np.power(factor,3) and mode == 1:\n","                  print(\"Stage 1 ended. No improvement in validation accuracy for %d epochs.\" % patience)\n","                  flag_start_again = 2\n","                  mode = 2\n","                  torch.save(model, path1)\n","                  break\n","\n","              if curr_lr == learning_rate*np.power(factor,3) and mode == 2:\n","                  print(\"Stage 2 ended. No improvement in validation accuracy for %d epochs.\" % patience)\n","                  mode = 3\n","                  break\n","\n","\n","              if tj == 10 and val_acc < 53:\n","                  flag_start_again = 1\n","\n","  del y_in\n","  del labels_in\n","  return losses, epoch_lst, train_accs, val_accs\n","\n","\n","def create_labels_Model_Attention(model,data,data_class,data_weigh,params):\n","\n","  data = (data - params[0])/params[1]\n","  data_class = (data_class - params[2])/params[3]\n","  data_weigh = (data_weigh - params[4])/params[5]\n","\n","  data = torch.Tensor(data)\n","  data_class = torch.Tensor(data_class)\n","  data_weigh = torch.Tensor(data_weigh)\n","  nn_values = model(data,data_class,data_weigh,2)\n","  labels_out = torch.round(nn_values)\n","\n","  return np.squeeze(labels_out.detach().numpy()), np.squeeze(nn_values.detach().numpy())\n","\n","def create_labels_Model_Concatenate(model,data,data_class,data_weigh,params):\n","\n","  data = (data - params[0])/params[1]\n","  data_class = (data_class - params[2])/params[3]\n","  data_weigh = (data_weigh - params[4])/params[5]\n","\n","  data = torch.Tensor(data)\n","  data_class = torch.Tensor(data_class)\n","  data_weigh = torch.Tensor(data_weigh)\n","  nn_values = model(data,data_class,data_weigh,1)\n","  labels_out = torch.round(nn_values)\n","\n","  return np.squeeze(labels_out.detach().numpy()), np.squeeze(nn_values.detach().numpy())\n","\n","\n","def estimate_accuracy_Model_Attention(model, data, labels, data_class, data_weigh, curr_mode = 2):\n","\n","    data = torch.Tensor(data)\n","    data_class = torch.Tensor(data_class)\n","    data_weigh = torch.Tensor(data_weigh)\n","    labels = torch.Tensor(labels).float().unsqueeze(1)\n","    out = model(data,data_class,data_weigh,curr_mode)\n","    correct_mask = torch.round(out) == labels\n","    correct = correct_mask.sum().item()\n","    all = correct_mask.numel()\n","\n","    return 100 * correct / all\n","\n","\n","def run_Model_Attention(labels_train,data_train,labels_val,data_val,labels_test,data_test,\n","                              class_feat_train,class_feat_val,class_feat_test,weigh_feat_train,weigh_feat_val,weigh_feat_test,\n","                              path1, path2, path3, learning_rate=1e-3, batch_size=50, epochs=100):\n","\n","  model = Model_Attention()\n","\n","  mu_dat, sigma_dat = find_normal_parameters(data_train)\n","  norm_train,norm_val,norm_test = normalize_datasets(data_train,data_val,data_test,mu_dat,sigma_dat)\n","\n","  mu_class, sigma_class = find_normal_parameters(class_feat_train)\n","  norm_train_class,norm_val_class,norm_test_class = normalize_datasets(class_feat_train,class_feat_val,class_feat_test,mu_class,sigma_class)\n","\n","  mu_weigh, sigma_weigh = find_normal_parameters(weigh_feat_train)\n","  norm_train_weigh,norm_val_weigh,norm_test_weigh = normalize_datasets(weigh_feat_train,weigh_feat_val,weigh_feat_test,mu_weigh,sigma_weigh)\n","\n","  learning_curve_info = train_Model_Attention(model,norm_train\n","                                                   , labels_train, norm_val\n","                                                   , labels_val,norm_train_class,norm_val_class,norm_train_weigh,norm_val_weigh\n","                                                   , learning_rate, batch_size, epochs, path1)\n","\n","  plot_learning_curve(*learning_curve_info)\n","\n","  test_accuracy = estimate_accuracy_Model_Attention(model, norm_test, labels_test, norm_test_class, norm_test_weigh,2)\n","  print(\"[Test Acc %.2f%%]\" % (test_accuracy))\n","\n","  torch.save(model, path2)\n","  np.save(path3,np.array([mu_dat,sigma_dat,mu_class,sigma_class,mu_weigh,sigma_weigh,WL_classifier,learning_rate,batch_size,epochs]))\n","\n","  return"],"metadata":{"id":"LBO6l2T-lwbV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#BM FCNN"],"metadata":{"id":"2A53gB549sA2"}},{"cell_type":"code","source":["class Model_BM_FCNN(nn.Module):\n","  def __init__(self,dim=WL_classifier):\n","    super().__init__()\n","    self.fct0 = nn.Linear(dim,model_classifier_layer1)\n","    self.fct = nn.Linear(model_classifier_layer1,model_classifier_layer2)\n","    self.fc1 = nn.Linear(model_classifier_layer2,model_classifier_layer3)\n","    self.fc2 = nn.Linear(model_classifier_layer3,model_classifier_layer4)\n","    self.fc3 = nn.Linear(model_classifier_layer4,6)\n","\n","  def forward(self,x):\n","    x = self.fct0(x)\n","    x = F.relu(x)\n","    x = self.fct(x)\n","    x = F.relu(x)\n","    x = self.fc1(x)\n","    x = F.relu(x)\n","    x = self.fc2(x)\n","    x = F.relu(x)\n","    x = self.fc3(x)\n","\n","    return x\n","\n","\n","def train_Model_BM_FCNN(model, train_data, train_labels, val_data, val_labels, learning_rate, batch_size, epochs, patience=20, factor=0.2):\n","\n","    flag_start_again = 0\n","    while(flag_start_again != 2):\n","      flag_start_again = 0\n","\n","      optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","      criterion = nn.CrossEntropyLoss()\n","      # Learning rate scheduler: ReduceLROnPlateau\n","      scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience, factor=factor, verbose=True)\n","\n","      losses, epoch_lst, train_accs, val_accs = [], [], [], []\n","      length_train = np.shape(train_data)[0]\n","      length_val = np.shape(val_data)[0]\n","\n","      best_val_acc = 0\n","      tj = 0\n","      flagy = 0\n","      for j in range(epochs):\n","          if flagy == 1 or flag_start_again == 1:\n","            break\n","          perm = np.random.permutation(length_train)\n","          train_data = train_data[perm]\n","          train_labels = train_labels[perm]\n","\n","          for i in range(0, length_train, batch_size):\n","              if (i + batch_size) > length_train:\n","                  break\n","              y_in = train_data[i:i+batch_size, :]\n","              labels_in = train_labels[i:i+batch_size]\n","              y_in = torch.Tensor(y_in)\n","              labels_in = torch.Tensor(labels_in).long()\n","              output = model(y_in)\n","              loss = criterion(output, labels_in)\n","              optimizer.zero_grad()\n","              loss.backward()\n","              optimizer.step()\n","\n","              if i % int(batch_size*300) == 0:\n","                losses.append(float(loss)/batch_size)  # compute *average* loss\n","                epoch_lst.append(tj)\n","                train_cost = float(loss.detach().numpy())\n","                train_acc = estimate_accuracy_Model_BM_FCNN(model, train_data, train_labels)\n","                train_accs.append(train_acc)\n","                val_acc = estimate_accuracy_Model_BM_FCNN(model, val_data, val_labels)\n","                val_accs.append(val_acc)\n","                print(\"Epoch %d. [Val Acc %.2f%%] [Train Acc %.2f%%, Loss %f]\" % (\n","                      j, val_acc, train_acc, train_cost))\n","                tj += 1\n","\n","                scheduler.step(val_acc)\n","                for param_group in optimizer.param_groups:\n","                    curr_lr = param_group['lr']\n","                if curr_lr == learning_rate*np.power(factor,3):\n","                    print(\"Training stopped. No improvement in validation accuracy for %d epochs.\" % patience)\n","                    flagy = 1\n","                    flag_start_again = 2\n","                    break\n","\n","                if tj == 10 and val_acc < 53:\n","                  flag_start_again = 1\n","\n","    del y_in\n","    del labels_in\n","    return losses, epoch_lst, train_accs, val_accs\n","\n","\n","def estimate_accuracy_Model_BM_FCNN(model, data, labels):\n","\n","    data = torch.Tensor(data)\n","    labels = torch.Tensor(labels).long()\n","    out = model(data)\n","    correct_mask = torch.argmax(out,axis=1) == labels\n","    correct = correct_mask.sum().item()\n","    all = correct_mask.numel()\n","\n","    return 100 * correct / all\n","\n","def create_labels_Model_BM_FCNN(model,data,params):\n","\n","  data = (data - params[0])/params[1]\n","  data = torch.Tensor(data)\n","  nn_values = model(data)\n","  labels_out = torch.round(nn_values)\n","\n","  return np.squeeze(labels_out.detach().numpy()), np.squeeze(nn_values.detach().numpy())\n","\n","\n","def run_Model_BM_FCNN(labels_train,data_train,labels_val,data_val,labels_test,data_test,path1, path2, learning_rate=1e-3, batch_size=50, epochs=100):\n","  model = Model_BM_FCNN()\n","\n","  mu, sigma = find_normal_parameters(data_train)\n","  norm_train,norm_val,norm_test = normalize_datasets(data_train,data_val,data_test,mu,sigma)\n","\n","  learning_curve_info = train_Model_BM_FCNN(model,norm_train\n","                                                   , labels_train, norm_val\n","                                                   , labels_val, learning_rate, batch_size, epochs)\n","\n","  plot_learning_curve(*learning_curve_info)\n","\n","  test_accuracy = estimate_accuracy_Model_BM_FCNN(model, norm_test, labels_test)\n","  print(\"[Test Acc %.2f%%]\" % (test_accuracy))\n","\n","  torch.save(model, path1)\n","  np.save(path2,np.array([mu,sigma,WL_classifier,learning_rate,batch_size,epochs]))\n","\n","  return"],"metadata":{"id":"ZLQCjNa3b6P3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#BM FCNN12"],"metadata":{"id":"FWi1Z3REb7RQ"}},{"cell_type":"code","source":["class Model_BM_FCNN1(nn.Module):\n","  def __init__(self,dim=WL_classifier):\n","    super().__init__()\n","    self.fct0 = nn.Linear(dim,model_classifier_layer1)\n","    self.fct = nn.Linear(model_classifier_layer1,model_classifier_layer2)\n","    self.fc1 = nn.Linear(model_classifier_layer2,model_classifier_layer3)\n","    self.fc2 = nn.Linear(model_classifier_layer3,model_classifier_layer4)\n","    self.fc3 = nn.Linear(model_classifier_layer4,model_classifier_layer5)\n","    self.fc4 = nn.Linear(model_classifier_layer5,1)\n","\n","  def forward(self,x):\n","    x = self.fct0(x)\n","    x = F.relu(x)\n","    x = self.fct(x)\n","    x = F.relu(x)\n","    x = self.fc1(x)\n","    x = F.relu(x)\n","    x = self.fc2(x)\n","    x = F.relu(x)\n","    x = self.fc3(x)\n","    x = F.relu(x)\n","    x = self.fc4(x)\n","    x = F.sigmoid(x)\n","\n","    return x\n","\n","\n","def train_Model_BM_FCNN1(model, train_data, train_labels, val_data, val_labels, learning_rate, batch_size, epochs, patience=20, factor=0.2):\n","\n","    flag_start_again = 0\n","    while(flag_start_again != 2):\n","      flag_start_again = 0\n","\n","      optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","      criterion = nn.BCELoss()\n","\n","      # Learning rate scheduler: ReduceLROnPlateau\n","      scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience, factor=factor, verbose=True)\n","\n","      losses, epoch_lst, train_accs, val_accs = [], [], [], []\n","      length_train = np.shape(train_data)[0]\n","      length_val = np.shape(val_data)[0]\n","\n","      best_val_acc = 0\n","      tj = 0\n","      flagy = 0\n","      for j in range(epochs):\n","          if flagy == 1 or flag_start_again == 1:\n","            break\n","          perm = np.random.permutation(length_train)\n","          train_data = train_data[perm]\n","          train_labels = train_labels[perm]\n","\n","          for i in range(0, length_train, batch_size):\n","              if (i + batch_size) > length_train:\n","                  break\n","              y_in = train_data[i:i+batch_size, :]\n","              labels_in = train_labels[i:i+batch_size]\n","              y_in = torch.Tensor(y_in)\n","              labels_in = torch.Tensor(labels_in).float().unsqueeze(1)\n","\n","              output = model(y_in)\n","              loss = criterion(output, labels_in)\n","              optimizer.zero_grad()\n","              loss.backward()\n","              optimizer.step()\n","\n","              if i % int(batch_size*300) == 0:\n","                losses.append(float(loss)/batch_size)  # compute *average* loss\n","                epoch_lst.append(tj)\n","                train_cost = float(loss.detach().numpy())\n","                train_acc = estimate_accuracy_Model_BM_FCNN1(model, train_data, train_labels)\n","                train_accs.append(train_acc)\n","                val_acc = estimate_accuracy_Model_BM_FCNN1(model, val_data, val_labels)\n","                val_accs.append(val_acc)\n","                print(\"Epoch %d. [Val Acc %.2f%%] [Train Acc %.2f%%, Loss %f]\" % (\n","                      j, val_acc, train_acc, train_cost))\n","                tj += 1\n","\n","                scheduler.step(val_acc)\n","                for param_group in optimizer.param_groups:\n","                    curr_lr = param_group['lr']\n","                if curr_lr == learning_rate*np.power(factor,3):\n","                    print(\"Training stopped. No improvement in validation accuracy for %d epochs.\" % patience)\n","                    flagy = 1\n","                    flag_start_again = 2\n","                    break\n","\n","                if tj == 10 and val_acc < 53:\n","                  flag_start_again = 1\n","\n","    del y_in\n","    del labels_in\n","    return losses, epoch_lst, train_accs, val_accs\n","\n","def estimate_accuracy_Model_BM_FCNN1(model, data, labels):\n","\n","    data = torch.Tensor(data)\n","    labels = torch.Tensor(labels).float().unsqueeze(1)\n","    out = model(data)\n","    correct_mask = torch.round(out) == labels\n","    correct = correct_mask.sum().item()\n","    all = correct_mask.numel()\n","\n","    return 100 * correct / all\n","\n","def create_labels_Model_BM_FCNN1(model,data,params):\n","\n","  data = (data - params[0])/params[1]\n","  data = torch.Tensor(data)\n","  nn_values = model(data)\n","  labels_out = torch.round(nn_values)\n","\n","  return np.squeeze(labels_out.detach().numpy()), np.squeeze(nn_values.detach().numpy())\n","\n","\n","def run_Model_BM_FCNN1(labels_train,data_train,labels_val,data_val,labels_test,data_test,path1, path2, learning_rate=1e-3, batch_size=50, epochs=100):\n","  model = Model_BM_FCNN1()\n","\n","  mu, sigma = find_normal_parameters(data_train)\n","  norm_train,norm_val,norm_test = normalize_datasets(data_train,data_val,data_test,mu,sigma)\n","\n","  learning_curve_info = train_Model_BM_FCNN1(model,norm_train\n","                                                   , labels_train, norm_val\n","                                                   , labels_val, learning_rate, batch_size, epochs)\n","\n","  plot_learning_curve(*learning_curve_info)\n","\n","  test_accuracy = estimate_accuracy_Model_BM_FCNN1(model, norm_test, labels_test)\n","  print(\"[Test Acc %.2f%%]\" % (test_accuracy))\n","\n","  torch.save(model, path1)\n","  np.save(path2,np.array([mu,sigma,WL_classifier,learning_rate,batch_size,epochs]))\n","\n","  return"],"metadata":{"id":"aDrssjxFb8qc"},"execution_count":null,"outputs":[]}]}