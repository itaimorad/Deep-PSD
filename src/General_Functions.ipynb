{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1iFO6f-25xnQSpVZHQbhd0f1xd8xsFnx0","timestamp":1694942833541}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"S-Iq3_J7g45d"},"outputs":[],"source":["def generate_discrete_data(data,arrival_times,arrival_labels,last_arrival_time, extra_features = False, rand_off = False, subtract_baseline = True, WL_classifier = 100):\n","\n","  coppied_arrival_times = np.copy(arrival_times)\n","\n","  if rand_off == True:\n","    rand_offset_lst = np.round(np.random.normal(0,max_random_offset/3,len(arrival_times) - 2)).astype(int)\n","    for i in range(len(rand_offset_lst)):\n","      if rand_offset_lst[i] <= -max_random_offset or rand_offset_lst[i] >= max_random_offset:\n","        rand_offset_lst[i] = 0\n","    coppied_arrival_times[2:len(arrival_times)] += rand_offset_lst\n","\n","  coppied_arrival_times = coppied_arrival_times.astype(int)\n","\n","  N_pulses = 0\n","  for i in range(len(coppied_arrival_times)):\n","    if coppied_arrival_times[i] >= cont_starting_time and coppied_arrival_times[i] < last_arrival_time and arrival_labels[i] != -1:\n","      N_pulses += 1\n","\n","  discrete_dataset = np.zeros((N_pulses,WL_classifier))\n","  discrete_labels = np.zeros(N_pulses)\n","  original_idx_lst = np.zeros(N_pulses)\n","\n","  if extra_features == False:\n","    j = 0\n","    for i in range(len(coppied_arrival_times)):\n","      if coppied_arrival_times[i] >= cont_starting_time and coppied_arrival_times[i] < last_arrival_time and arrival_labels[i] != -1:\n","        if subtract_baseline == 'True':\n","          discrete_dataset[j,:] = data[coppied_arrival_times[i] - time_offset:coppied_arrival_times[i] - time_offset + WL_classifier] - data[coppied_arrival_times[i] - time_offset-1]\n","        else:\n","          discrete_dataset[j,:] = data[coppied_arrival_times[i] - time_offset:coppied_arrival_times[i] - time_offset + WL_classifier]\n","        discrete_labels[j] = arrival_labels[i]\n","        original_idx_lst[j] = i\n","        j += 1\n","\n","    return discrete_dataset, discrete_labels, original_idx_lst\n","\n","  else:\n","\n","    classification_features = np.zeros((len(coppied_arrival_times),num_classification_features))\n","    attention_features = np.zeros((len(coppied_arrival_times),num_attention_features))\n","    classification_features_out = np.zeros((N_pulses,num_classification_features))\n","    attention_features_out = np.zeros((N_pulses,num_attention_features))\n","\n","\n","    for i in range(1,len(coppied_arrival_times) - 1):\n","\n","\n","      pulse = data[coppied_arrival_times[i] - new_starting_point:coppied_arrival_times[i] + pulse_len_after_starting] - data[coppied_arrival_times[i] - 1]\n","      pulse_bef = data[coppied_arrival_times[i-1] - new_starting_point:coppied_arrival_times[i-1] + pulse_len_after_starting] - data[coppied_arrival_times[i-1] - 1]\n","      pulse_aft = data[coppied_arrival_times[i+1] - new_starting_point:coppied_arrival_times[i+1] + pulse_len_after_starting] - data[coppied_arrival_times[i+1] - 1]\n","      delta_t_pulse_aft = coppied_arrival_times[i+1] - coppied_arrival_times[i]\n","      delta_t_pulse_bef = coppied_arrival_times[i] - coppied_arrival_times[i-1]\n","\n","      t_rise2,amp2,qs,ql,psd_ratio,corr_n,corr_g = calc_specific_features_for_pulse(pulse,pulse_bef,pulse_aft, template_n,template_g, new_starting_point, pulse_len_after_starting,delta_t_pulse_aft)\n","\n","      baseline = data[coppied_arrival_times[i] - 1]\n","\n","      # to make sure we dont divide by 0.\n","      if qs == 0:\n","        qs = 1e-10\n","      if ql == 0:\n","        ql = 1e-10\n","      if amp2 == 0:\n","        amp2 = 1e-10\n","      if baseline == 0:\n","        baseline = 1e-10\n","\n","      classification_features[i,:] = [t_rise2,amp2,qs,ql,psd_ratio,corr_n,corr_g,baseline]\n","\n","    for i in range(2,len(coppied_arrival_times) - 2):\n","\n","      delta_t_pulse_aft = coppied_arrival_times[i+1] - coppied_arrival_times[i]\n","      delta_t_pulse_bef = coppied_arrival_times[i] - coppied_arrival_times[i-1]\n","      baseline = classification_features[i,7]\n","\n","      ql_ratio_before = classification_features[i,3]/classification_features[i-1,3]\n","      ql_baseline_ratio = classification_features[i,3]/baseline\n","      ql_ratio_after = classification_features[i,3]/classification_features[i+1,3]\n","      qs_ratio_before = classification_features[i,2]/classification_features[i-1,2]\n","      qs_ratio_after = classification_features[i,2]/classification_features[i+1,2]\n","\n","      attention_features[i,:] = [delta_t_pulse_aft,delta_t_pulse_bef,baseline,ql_ratio_before,ql_baseline_ratio,ql_ratio_after,qs_ratio_before,\n","                                qs_ratio_after]\n","\n","    j = 0\n","    for i in range(len(coppied_arrival_times)):\n","      if coppied_arrival_times[i] >= cont_starting_time and coppied_arrival_times[i] < last_arrival_time and arrival_labels[i] != -1:\n","        if subtract_baseline == 'True':\n","          discrete_dataset[j,:] = data[coppied_arrival_times[i] - time_offset:coppied_arrival_times[i] - time_offset + WL_classifier] - data[coppied_arrival_times[i] - time_offset-1]\n","        else:\n","          discrete_dataset[j,:] = data[coppied_arrival_times[i] - time_offset:coppied_arrival_times[i] - time_offset + WL_classifier]\n","        discrete_labels[j] = arrival_labels[i]\n","        original_idx_lst[j] = i\n","        classification_features_out[j,:] = classification_features[i,:]\n","        attention_features_out[j,:] = attention_features[i,:]\n","        j += 1\n","\n","    return discrete_dataset, discrete_labels, original_idx_lst,classification_features_out,attention_features_out\n","\n","def generate_discrete_data_with_FA(data,arrival_times,arrival_labels,last_arrival_time, extra_features = False, rand_off = False, subtract_baseline = True, WL_classifier = 100):\n","\n","  coppied_arrival_times = np.copy(arrival_times)\n","\n","  if rand_off == True:\n","    rand_offset_lst = np.round(np.random.normal(0,max_random_offset/3,len(arrival_times) - 2)).astype(int)\n","    for i in range(len(rand_offset_lst)):\n","      if rand_offset_lst[i] <= -max_random_offset or rand_offset_lst[i] >= max_random_offset:\n","        rand_offset_lst[i] = 0\n","    coppied_arrival_times[2:len(arrival_times)] += rand_offset_lst\n","\n","  coppied_arrival_times = coppied_arrival_times.astype(int)\n","\n","  N_pulses = 0\n","  for i in range(len(coppied_arrival_times)):\n","    if coppied_arrival_times[i] >= cont_starting_time and coppied_arrival_times[i] < last_arrival_time:\n","      N_pulses += 1\n","\n","  discrete_dataset = np.zeros((N_pulses,WL_classifier))\n","  discrete_labels = np.zeros(N_pulses)\n","  original_idx_lst = np.zeros(N_pulses)\n","\n","  if extra_features == False:\n","    j = 0\n","    for i in range(len(coppied_arrival_times)):\n","      if coppied_arrival_times[i] >= cont_starting_time and coppied_arrival_times[i] < last_arrival_time:\n","        if subtract_baseline == 'True':\n","          discrete_dataset[j,:] = data[coppied_arrival_times[i] - time_offset:coppied_arrival_times[i] - time_offset + WL_classifier] - data[coppied_arrival_times[i] - time_offset-1]\n","        else:\n","          discrete_dataset[j,:] = data[coppied_arrival_times[i] - time_offset:coppied_arrival_times[i] - time_offset + WL_classifier]\n","        discrete_labels[j] = arrival_labels[i]\n","        original_idx_lst[j] = i\n","        j += 1\n","\n","    return discrete_dataset, discrete_labels, original_idx_lst\n","\n","  else:\n","\n","    classification_features = np.zeros((len(coppied_arrival_times),num_classification_features))\n","    attention_features = np.zeros((len(coppied_arrival_times),num_attention_features))\n","    classification_features_out = np.zeros((N_pulses,num_classification_features))\n","    attention_features_out = np.zeros((N_pulses,num_attention_features))\n","\n","\n","    for i in range(1,len(coppied_arrival_times) - 1):\n","\n","      pulse = data[coppied_arrival_times[i] - new_starting_point:coppied_arrival_times[i] + pulse_len_after_starting] - data[coppied_arrival_times[i] - 1]\n","      pulse_bef = data[coppied_arrival_times[i-1] - new_starting_point:coppied_arrival_times[i-1] + pulse_len_after_starting] - data[coppied_arrival_times[i-1] - 1]\n","      pulse_aft = data[coppied_arrival_times[i+1] - new_starting_point:coppied_arrival_times[i+1] + pulse_len_after_starting] - data[coppied_arrival_times[i+1] - 1]\n","      delta_t_pulse_aft = coppied_arrival_times[i+1] - coppied_arrival_times[i]\n","      delta_t_pulse_bef = coppied_arrival_times[i] - coppied_arrival_times[i-1]\n","\n","      t_rise2,amp2,qs,ql,psd_ratio,corr_n,corr_g = calc_specific_features_for_pulse(pulse,pulse_bef,pulse_aft, template_n,template_g, new_starting_point, pulse_len_after_starting,delta_t_pulse_aft)\n","\n","      baseline = data[coppied_arrival_times[i] - 1]\n","\n","      # to make sure we dont divide by 0.\n","      if qs == 0:\n","        qs = 1e-10\n","      if ql == 0:\n","        ql = 1e-10\n","      if amp2 == 0:\n","        amp2 = 1e-10\n","      if baseline == 0:\n","        baseline = 1e-10\n","      classification_features[i,:] = [t_rise2,amp2,qs,ql,psd_ratio,corr_n,corr_g,baseline]\n","\n","    for i in range(2,len(coppied_arrival_times) - 2):\n","\n","      delta_t_pulse_aft = coppied_arrival_times[i+1] - coppied_arrival_times[i]\n","      delta_t_pulse_bef = coppied_arrival_times[i] - coppied_arrival_times[i-1]\n","      baseline = classification_features[i,7]\n","\n","      ql_ratio_before = classification_features[i,3]/classification_features[i-1,3]\n","      ql_baseline_ratio = classification_features[i,3]/baseline\n","      ql_ratio_after = classification_features[i,3]/classification_features[i+1,3]\n","      qs_ratio_before = classification_features[i,2]/classification_features[i-1,2]\n","      qs_ratio_after = classification_features[i,2]/classification_features[i+1,2]\n","\n","      attention_features[i,:] = [delta_t_pulse_aft,delta_t_pulse_bef,baseline,ql_ratio_before,ql_baseline_ratio,ql_ratio_after,qs_ratio_before,\n","                                qs_ratio_after]\n","\n","    j = 0\n","    for i in range(len(coppied_arrival_times)):\n","      if coppied_arrival_times[i] >= cont_starting_time and coppied_arrival_times[i] < last_arrival_time:\n","        if subtract_baseline == 'True':\n","          discrete_dataset[j,:] = data[coppied_arrival_times[i] - time_offset:coppied_arrival_times[i] - time_offset + WL_classifier] - data[coppied_arrival_times[i] - time_offset-1]\n","        else:\n","          discrete_dataset[j,:] = data[coppied_arrival_times[i] - time_offset:coppied_arrival_times[i] - time_offset + WL_classifier]\n","        discrete_labels[j] = arrival_labels[i]\n","        original_idx_lst[j] = i\n","        classification_features_out[j,:] = classification_features[i,:]\n","        attention_features_out[j,:] = attention_features[i,:]\n","        j += 1\n","\n","    return discrete_dataset, discrete_labels, original_idx_lst,classification_features_out,attention_features_out\n","\n","\n","\n","def generate_pulse_train(neutrons,gammas,lambda_n,lambda_g, return_statistics = False, add_noise = False):\n","\n","  N_n, len_neutron = np.shape(neutrons)\n","  N_g, len_gamma = np.shape(gammas)\n","  max_length = max([len_neutron,len_gamma])\n","  n_exp_var_list = np.random.exponential(round(1/(Ts*lambda_n)), int(N_n/g_n_ratio))\n","  n_pulse_starting_points = np.array([int(round(sum(n_exp_var_list[:x]))) for x in range(1,int(N_n/g_n_ratio)+1)])\n","  g_exp_var_list = np.random.exponential(round(1/(Ts*lambda_g)), N_g)\n","  g_pulse_starting_points = np.array([int(round(sum(g_exp_var_list[:x]))) for x in range(1,N_g+1)])\n","  last_neutron_arriving_time = n_pulse_starting_points[len(n_pulse_starting_points)-1]\n","  last_gamma_arriving_time = g_pulse_starting_points[len(g_pulse_starting_points)-1]\n","  last_arrival_time = min(last_neutron_arriving_time,last_gamma_arriving_time)\n","  total_time = max(last_neutron_arriving_time,last_gamma_arriving_time) + 5*max_length + 1\n","  labels = np.zeros((total_time))\n","\n","  for index in n_pulse_starting_points:\n","    while labels[index] != 0:\n","      index  += 1\n","    labels[index] = 1\n","\n","  for index in g_pulse_starting_points:\n","    while labels[index] != 0:\n","      index  += 1\n","    labels[index] = 2\n","\n","  if return_statistics == True:\n","\n","    len_now = np.sum(labels != 0)\n","    true_inf = np.zeros((len_now,len_true_inf))\n","    neutron_features = generate_features_original_pulses(neutrons, template_n, template_g)\n","    gamma_features = generate_features_original_pulses(gammas, template_n, template_g)\n","    data = np.zeros((total_time))\n","    k,r,j = 0,0,0\n","\n","    for i in range(len(labels)):\n","      if labels[i] == 1:\n","        data[i:i + max_length - starting_index] += neutrons[k,starting_index:]\n","        true_inf[j,0] = i # arrival location\n","        true_inf[j,1] = 0 # label\n","        true_inf[j,2] = k # serial number of neutron\n","        true_inf[j,3:3+num_original_features] = neutron_features[k,:]\n","        k += 1\n","        j += 1\n","      if labels[i] == 2:\n","        data[i:i + max_length - starting_index] += gammas[r,starting_index:]\n","        true_inf[j,0] = i\n","        true_inf[j,1] = 1\n","        true_inf[j,2] = r\n","        true_inf[j,3:3+num_original_features] = gamma_features[r,:]\n","        r += 1\n","        j += 1\n","\n","    k,r,j = 0,0,0\n","\n","    for i in range(len(labels)):\n","      if labels[i] == 1:\n","        if j != 0 and j != len_now - 1:\n","          true_inf[j,num_original_features + 3] = true_inf[j,0] - true_inf[j-1,0]\n","          true_inf[j,num_original_features + 4] = true_inf[j+1,0] - true_inf[j,0]\n","          true_inf[j,num_original_features + 5] = true_inf[j,11]/true_inf[j-1,11]\n","          true_inf[j,num_original_features + 6] = true_inf[j,11]/true_inf[j+1,11]\n","          true_inf[j,num_original_features + 7] = true_inf[j,12]/true_inf[j-1,12]\n","          true_inf[j,num_original_features + 8] = true_inf[j,12]/true_inf[j+1,12]\n","\n","        k += 1\n","        j += 1\n","      if labels[i] == 2:\n","        if j != 0 and j != len_now - 1:\n","          true_inf[j,num_original_features + 3] = true_inf[j,0] - true_inf[j-1,0]\n","          true_inf[j,num_original_features + 4] = true_inf[j+1,0] - true_inf[j,0]\n","          true_inf[j,num_original_features + 5] = true_inf[j,11]/true_inf[j-1,11]\n","          true_inf[j,num_original_features + 6] = true_inf[j,11]/true_inf[j+1,11]\n","          true_inf[j,num_original_features + 7] = true_inf[j,12]/true_inf[j-1,12]\n","          true_inf[j,num_original_features + 8] = true_inf[j,12]/true_inf[j+1,12]\n","        r += 1\n","        j += 1\n","\n","  else:\n","    len_now = np.sum(labels != 0)\n","    true_inf = np.zeros((len_now,2))\n","    data = np.zeros((total_time))\n","    k,r,j = 0,0,0\n","\n","    for i in range(len(labels)):\n","      if labels[i] == 1:\n","        data[i:i + max_length - starting_index] += neutrons[k,starting_index:]\n","        true_inf[j,0] = i # arrival location\n","        true_inf[j,1] = 0 # label, 0 for neutrons\n","        k += 1\n","        j += 1\n","      if labels[i] == 2:\n","        data[i:i + max_length - starting_index] += gammas[r,starting_index:]\n","        true_inf[j,0] = i # arrival location\n","        true_inf[j,1] = 1 # label, 1 for gammas\n","        r += 1\n","        j += 1\n","\n","  if add_noise == True:\n","    wgn = np.random.normal(noise_mean, std_added_noise, size=len(data))\n","    data = data + wgn\n","\n","  return data,labels,true_inf,last_arrival_time\n","\n","def generate_pulse_train_peak_labels(neutrons,gammas,lambda_n,lambda_g, add_noise = False):\n","\n","  N_n, len_neutron = np.shape(neutrons)\n","  N_g, len_gamma = np.shape(gammas)\n","  max_length = max([len_neutron,len_gamma])\n","  n_exp_var_list = np.random.exponential(round(1/(Ts*lambda_n)), int(N_n/g_n_ratio))\n","  n_pulse_starting_points = np.array([int(round(sum(n_exp_var_list[:x]))) for x in range(1,int(N_n/g_n_ratio)+1)])\n","  g_exp_var_list = np.random.exponential(round(1/(Ts*lambda_g)), N_g)\n","  g_pulse_starting_points = np.array([int(round(sum(g_exp_var_list[:x]))) for x in range(1,N_g+1)])\n","  last_neutron_arriving_time = n_pulse_starting_points[int(N_n/g_n_ratio)-1]\n","  last_gamma_arriving_time = g_pulse_starting_points[N_g-1]\n","  last_arrival_time = min(last_neutron_arriving_time,last_gamma_arriving_time)\n","  total_time = max(last_neutron_arriving_time,last_gamma_arriving_time) + 3*max_length + 1\n","  labels = np.zeros((total_time))\n","\n","  for index in n_pulse_starting_points:\n","    while labels[index] != 0:\n","      index  += 1\n","    labels[index] = 1\n","\n","  for index in g_pulse_starting_points:\n","    while labels[index] != 0:\n","      index  += 1\n","    labels[index] = 2\n","\n","  len_now = np.sum(labels != 0)\n","  true_inf = np.zeros((len_now,2))\n","  data = np.zeros((total_time))\n","  peak_labels = np.zeros(len(labels))\n","  true_inf_peaks = np.zeros((len_now,2))\n","  k,r,j = 0,0,0\n","\n","  for i in range(len(labels)):\n","    if labels[i] == 1:\n","      data[i:i + max_length - starting_index] += neutrons[k,starting_index:]\n","      true_inf[j,0] = i # arrival location\n","      true_inf[j,1] = 0 # label\n","      peak_idx = int(np.argmax(neutrons[k,starting_index:starting_index + 200]) + starting_index)\n","      while(peak_labels[i + peak_idx] != 0):\n","        i += 1\n","      peak_labels[i + peak_idx] = 1\n","      true_inf_peaks[j,0] = i + peak_idx # arrival location\n","      true_inf_peaks[j,1] = 0 # label\n","      k += 1\n","      j += 1\n","    if labels[i] == 2:\n","      data[i:i + max_length - starting_index] += gammas[r,starting_index:]\n","      true_inf[j,0] = i\n","      true_inf[j,1] = 1\n","      peak_idx = int(np.argmax(gammas[r,starting_index:starting_index + 200]) + starting_index)\n","      while(peak_labels[i + peak_idx] != 0):\n","        i += 1\n","      peak_labels[i + peak_idx] = 1\n","      true_inf_peaks[j,0] = i + peak_idx # arrival location\n","      true_inf_peaks[j,1] = 1 # label\n","      r += 1\n","      j += 1\n","\n","  if add_noise == True:\n","    wgn = np.random.normal(noise_mean, std_added_noise, size=len(data))\n","    data = data + wgn\n","\n","  return data,labels,true_inf,peak_labels,true_inf_peaks,last_arrival_time\n","\n","def correlation_coeff(x,y):\n","  sigma_x = np.std(x)\n","  sigma_y = np.std(y)\n","  mean_x = np.mean(x)\n","  mean_y = np.mean(y)\n","  N = len(x)\n","  sum = 0\n","  for i in range(N):\n","    sum += (x[i] - mean_x)*(y[i] - mean_y)\n","  sum = sum/(N*sigma_x*sigma_y)\n","\n","  return sum\n","\n","def generate_features_original_pulses(data, template_pulse_n, template_pulse_g):\n","\n","  s1,s2 = np.shape(data)\n","  deriv_data = np.zeros((s1,L_deriv_orig_features))\n","  deriv_2_data = np.zeros((s1,L_deriv_orig_features))\n","  t_amp1 = np.zeros(s1,dtype=int) # amplitude index based on 2nd derivative\n","  t_rise1 = np.zeros(s1,dtype=int) # rise time based on first method amplitude\n","  t_amp2 = np.zeros(s1,dtype=int) # amplitude index based on maximum value\n","  t_rise2 = np.zeros(s1,dtype=int) # rise time based on second method amplitude\n","  qs = np.zeros(s1)\n","  ql = np.zeros(s1)\n","  psd_ratio = np.zeros(s1)\n","  amp1 = np.zeros(s1) # amplitude based on first method amplitude\n","  amp2 = np.zeros(s1) # amplitude based on second method amplitude\n","  tau1 = np.zeros(s1,dtype=int)# decay constant based on first method amplitude\n","  tau2 = np.zeros(s1,dtype=int) # decay constant based on second method amplitude\n","  corr_g = np.zeros(s1)\n","  corr_n = np.zeros(s1)\n","\n","  count1 = 0\n","  for i in range(s1):\n","    for idx in range(L_deriv_orig_features):\n","      deriv_data[i,idx] = (data[i][idx+starting_index] - data[i][idx+starting_index-delta_orig_features])/delta_orig_features\n","      deriv_2_data[i,idx] = (deriv_data[i][idx] - ((data[i][idx+starting_index-delta_orig_features] - data[i][idx+starting_index-2*delta_orig_features])/delta_orig_features))/delta_orig_features\n","\n","    flag1 = 0\n","    for idx in range(10,L_deriv_orig_features):\n","      if deriv_data[i,idx - 1] > 0 and deriv_data[i,idx] < 0 and deriv_2_data[i,idx] > 0:\n","        t_amp1[i] = idx + starting_index\n","        flag1 = 1\n","        break\n","    if flag1 == 0:\n","      count1 += 1\n","\n","    amp1[i] = data[i,t_amp1[i]]\n","    low_tr = amp1[i]*0.1\n","    high_tr = amp1[i]*0.9\n","\n","    flag1 = 0\n","    for idx in range(starting_index,starting_index + L_deriv_orig_features):\n","      if data[i,idx] > low_tr and flag1 == 0:\n","        time1 = idx\n","        flag1 = 1\n","      if data[i,idx] > high_tr:\n","        t_rise1[i] = idx - time1\n","        break\n","\n","    t_amp2[i] = np.argmax(data[i,starting_index:starting_index + 200]) + starting_index\n","\n","    amp2[i] = data[i,t_amp2[i]]\n","    low_tr = amp2[i]*0.1\n","    high_tr = amp2[i]*0.9\n","\n","    flag1 = 0\n","    for idx in range(starting_index,L_deriv_orig_features+starting_index):\n","      if data[i,idx] > low_tr and flag1 == 0:\n","        time1 = idx\n","        flag1 = 1\n","      if data[i,idx] > high_tr:\n","        t_rise2[i] = idx - time1\n","        break\n","\n","    for idx in range(t_amp1[i],s2):\n","      if data[i,idx] < amp1[i]*0.368:\n","        tau1[i] = idx - t_amp1[i]\n","        break\n","\n","    for idx in range(t_amp2[i],s2):\n","      if data[i,idx] < amp2[i]*0.368:\n","        tau2[i] = idx - t_amp2[i]\n","        break\n","\n","    qs[i] = np.sum(data[i,starting_index:starting_index+short_gate_orig_features])\n","    ql[i] = np.sum(data[i,starting_index:starting_index+long_gate_orig_features])\n","    psd_ratio[i] = (ql[i] - qs[i])/ql[i]\n","\n","    corr_n[i] = correlation_coeff(data[i,starting_index:starting_index+template_length_orig_features],template_pulse_n[:template_length_orig_features])\n","    corr_g[i] = correlation_coeff(data[i,starting_index:starting_index+template_length_orig_features],template_pulse_g[:template_length_orig_features])\n","\n","  output = np.concatenate((t_amp1[:,None],t_rise1[:,None],amp1[:,None],tau1[:,None],t_amp2[:,None],\n","                             t_rise2[:,None],amp2[:,None],tau2[:,None],qs[:,None],ql[:,None],psd_ratio[:,None],corr_n[:,None],corr_g[:,None]), axis=1)\n","\n","  return output\n","\n","def calc_specific_features_for_pulse(pulse,pulse_bef,pulse_aft, template_n, template_g, new_starting_point, pulse_len_after_starting,delta_t_pulse_aft):\n","\n","  if delta_t_pulse_aft > 200:\n","    t_amp2 = np.argmax(pulse[new_starting_point:new_starting_point + 200]) + new_starting_point\n","  if delta_t_pulse_aft > 100 and delta_t_pulse_aft <= 200:\n","    t_amp2 = np.argmax(pulse[new_starting_point:new_starting_point + 100]) + new_starting_point\n","  if delta_t_pulse_aft <= 100:\n","    if delta_t_pulse_aft > 1:\n","      t_amp2 = np.argmax(pulse[new_starting_point:new_starting_point + delta_t_pulse_aft - 1]) + new_starting_point\n","    else:\n","      t_amp2 = 0\n","\n","  amp2 = pulse[t_amp2]\n","  low_tr = amp2*0.1\n","  high_tr = amp2*0.9\n","\n","  flag3 = 0\n","  flag1 = 0\n","  for idx in range(new_starting_point,pulse_len_after_starting+new_starting_point):\n","    time1 = 0\n","    if pulse[idx] > low_tr and flag1 == 0:\n","      time1 = idx\n","      flag1 = 1\n","    if pulse[idx] > high_tr:\n","      t_rise2 = idx - time1\n","      flag3 = 1\n","      break\n","  if flag3 == 0:\n","    t_rise2 = 0\n","\n","  qs = np.sum(pulse[new_starting_point + start_gate_class_features:new_starting_point + start_gate_class_features+short_gate_class_features])\n","  ql = np.sum(pulse[new_starting_point + start_gate_class_features:new_starting_point + start_gate_class_features+long_gate_class_features])\n","  psd_ratio = (ql - qs)/ql\n","\n","  corr_n = correlation_coeff(pulse[new_starting_point + time_offset + corr_start_class_features:new_starting_point + time_offset + corr_end_class_features],template_n[corr_start_class_features:corr_end_class_features])\n","  corr_g = correlation_coeff(pulse[new_starting_point + time_offset + corr_start_class_features:new_starting_point + time_offset + corr_end_class_features],template_g[corr_start_class_features:corr_end_class_features])\n","\n","\n","  return t_rise2,amp2,qs,ql,psd_ratio,corr_n,corr_g\n","\n","def plot_data(data):\n","  pyplot.figure()\n","  pyplot.plot(np.arange(0,len(data)),data)\n","  pyplot.title('pulses')\n","  pyplot.xlabel('Time [ns]')\n","  pyplot.ylabel('Amplitude [mV]')\n","  pyplot.show()\n","\n","def plot_learning_curve(losses, epoch_lst, train_accs, val_accs):\n","  \"\"\"\n","  Plot the learning curve.\n","  \"\"\"\n","  plt.title(\"Learning Curve: Loss per Iteration\")\n","  plt.plot(epoch_lst, losses, label=\"Train\")\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(\"Loss\")\n","  plt.show()\n","\n","  plt.title(\"Learning Curve: Accuracy per Iteration\")\n","  plt.plot(epoch_lst, train_accs, label=\"Train\")\n","  plt.plot(epoch_lst, val_accs, label=\"Validation\")\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(\"Accuracy\")\n","  plt.legend(loc='best')\n","  plt.show()\n","\n","def find_normal_parameters(data):\n","  mue = np.mean(data)\n","  sigma = np.std(data)\n","  return mue, sigma\n","\n","def normalize_datasets(train,val,test,mue,sigma):\n","  norm_train = (train - mue)/sigma\n","  norm_val = (val - mue)/sigma\n","  norm_test = (test - mue)/sigma\n","\n","  return norm_train,norm_val,norm_test\n","\n","def create_arrivals_and_triggers(labels_in,labels_out,low,high,end):\n","\n","  ## create arrays of the arrival times, trigger times, and corresponding labels, from the\n","  ## continuous time observations of both the true detector signal labels and the method triggers.\n","  non_zero_indices = np.where((labels_out[cont_starting_time + margin - low:end - margin + high] != 0))[0]\n","  triggers = non_zero_indices + cont_starting_time + margin - low\n","  non_zero_indices = np.where((labels_in[cont_starting_time + margin:end - margin] != 0))[0]\n","  arrivals = non_zero_indices + cont_starting_time + margin\n","  mask = (labels_in[low:high] != 0)\n","  class_labels = np.where(labels_in[low:high][mask] == 1, 1, 2)\n","\n","  return arrivals,triggers,class_labels\n","\n","def match_pairs(arrivals,triggers,low,high):\n","\n","  d_range = int(low + high)\n","  ptr1 = 0\n","  ptr2 = 0\n","  triggered_events = -1*np.ones(len(arrivals))\n","  false_triggers_array = -1*np.ones(len(triggers))\n","  while(ptr1 < (len(arrivals) - 5) and ptr2 < (len(triggers) - 5)):\n","    if arrivals[ptr1 + 1] - arrivals[ptr1] > d_range:\n","      min_trig = arrivals[ptr1] - low\n","      max_trig = arrivals[ptr1] + high\n","      if triggers[ptr2] > max_trig:\n","        ptr1 += 1\n","        continue\n","      flag = 0\n","      ptr_new = ptr2\n","      curr_trigs = []\n","\n","      while(triggers[ptr_new] <= max_trig):\n","        if triggers[ptr_new] < min_trig:\n","          ptr2 += 1\n","          ptr_new += 1\n","        else:\n","          curr_trigs.append(triggers[ptr_new])\n","          ptr_new += 1\n","      if curr_trigs == []:\n","        ptr1 += 1\n","        continue\n","      else:\n","        min_dist = abs(curr_trigs[0] - arrivals[ptr1])\n","        min_idx = 0\n","        for i in range(len(curr_trigs)):\n","          if abs(curr_trigs[i] - arrivals[ptr1]) < min_dist:\n","            min_dist = abs(curr_trigs[i] - arrivals[ptr1])\n","            min_idx = i\n","        triggered_events[ptr1] = triggers[ptr2 + min_idx]\n","        false_triggers_array[ptr2 + min_idx] = arrivals[ptr1]\n","        ptr2 = ptr_new\n","        ptr1 += 1\n","\n","    else:\n","      flag = 0\n","      ptr_new1 = ptr1\n","      curr_arr = [arrivals[ptr_new1]]\n","      curr_trigs = []\n","      while(flag == 0):\n","        if arrivals[ptr_new1 + 1] - arrivals[ptr_new1] <= d_range:\n","          curr_arr.append(arrivals[ptr_new1 + 1])\n","          ptr_new1 += 1\n","        else:\n","          flag = 1\n","\n","      min_trig = curr_arr[0] - low\n","      max_trig = curr_arr[len(curr_arr) - 1] + high\n","      if triggers[ptr2] > max_trig:\n","        ptr1 = ptr_new1 + 1\n","        continue\n","      else:\n","        ptr_new2 = ptr2\n","        while(triggers[ptr_new2] <= max_trig):\n","          if triggers[ptr_new2] < min_trig:\n","            ptr2 += 1\n","            ptr_new2 += 1\n","          else:\n","            curr_trigs.append(triggers[ptr_new2])\n","            ptr_new2 += 1\n","        out1, out_indexes = find_optimal_pairs(curr_arr, curr_trigs, -low, high)\n","        for i in range(np.shape(out_indexes)[1]):\n","          triggered_events[int(ptr1 + out_indexes[0,i])] = triggers[int(ptr2 + out_indexes[1,i])]\n","          false_triggers_array[int(ptr2 + out_indexes[1,i])] = arrivals[int(ptr1 + out_indexes[0,i])]\n","        ptr1 = ptr_new1 + 1\n","        ptr2 = ptr_new2\n","\n","  return arrivals[:ptr1], triggers[:ptr2], triggered_events[:ptr1], false_triggers_array[:ptr2]\n","\n","def pair_distance(pair):\n","    return pair[1] - pair[0]\n","\n","def find_optimal_pairs(arr1, arr2, low, high):\n","\n","    min_distance_sum = float('inf')\n","    optimal_pairs = []\n","    optimal_indexes = []\n","    min_len = min(len(arr1),len(arr2))\n","    for r in range(1,min_len + 1)[::-1]:\n","      all_perms_arr1 = []\n","      all_perms_arr1.extend(permutations(arr1, r))\n","      all_perms_arr2 = []\n","      all_perms_arr2.extend(permutations(arr2, r))\n","      for perm1 in all_perms_arr1:\n","          for perm2 in all_perms_arr2:\n","              pairs = list(zip(enumerate(perm1), enumerate(perm2)))\n","              if all(low <= pair_distance((pair[0][1], pair[1][1])) <= high for pair in pairs):\n","                  distance_sum = sum(abs(pair_distance((pair[0][1], pair[1][1]))) for pair in pairs)\n","                  if distance_sum < min_distance_sum:\n","                      min_distance_sum = distance_sum\n","                      optimal_pairs = [(pair[0][1], pair[1][1]) for pair in pairs]\n","                      optimal_indexes = [(pair[0][0], pair[1][0]) for pair in pairs]\n","      if optimal_pairs != []:\n","        break\n","\n","    output_indexes = np.zeros((2,len(optimal_pairs)))\n","    for i in range(len(optimal_pairs)):\n","      opt1 = optimal_pairs[i][0]\n","      opt2 = optimal_pairs[i][1]\n","      for j in range(len(arr1)):\n","        if arr1[j] == opt1:\n","          arr1[j] = -1\n","          output_indexes[0,i] = j\n","          break\n","      for j in range(len(arr2)):\n","        if arr2[j] == opt2:\n","          arr2[j] = -1\n","          output_indexes[1,i] = j\n","          break\n","\n","    return np.array(optimal_pairs), output_indexes\n","\n","def calculate_statistics(triggered_events,false_triggers_array):\n","  all_counts = len(triggered_events) # Number of true events\n","  MD = np.count_nonzero(triggered_events == -1) # Number of miss-detections\n","  FA = np.count_nonzero(false_triggers_array == -1) # Number of false alarms\n","  tp = all_counts - MD # Number of true positives\n","\n","  if tp + FA == 0:\n","    precision = 0\n","  else:\n","    precision = tp/(tp+FA)\n","\n","  if tp + MD == 0:\n","    recall = 0\n","  else:\n","    recall = tp/(tp+MD)\n","\n","  if precision + recall == 0:\n","    F1_score = 0\n","  else:\n","    F1_score = 2*precision*recall/(precision + recall)\n","\n","  return all_counts,MD,FA,precision,recall,F1_score\n","\n","def calc_count_information(true_labels,detected_labels,end_time,low,high):\n","  arrivals,triggers,class_labels = create_arrivals_and_triggers(true_labels,detected_labels,low,high,end_time)\n","  arrivals_out, triggers_out, triggered_events_out, false_triggers_array_out = match_pairs(arrivals,triggers,low,high)\n","  all_counts,MD,FA,precision,recall,F1_score = calculate_statistics(triggered_events_out,false_triggers_array_out)\n","\n","  return all_counts,MD,FA,precision,recall,F1_score\n","\n","\n","def generate_template(pulses,template_length):\n","\n","  s1,s2 = np.shape(pulses)\n","  template_pulse = np.zeros(template_length)\n","  for i in range(s1):\n","    template_pulse += pulses[i][starting_index:starting_index + template_length]\n","  template_pulse = template_pulse/max(template_pulse)\n","\n","  return template_pulse"]},{"cell_type":"code","source":["def calc_deep_trigger_window_statistics(arrivals,triggers,trig_labels,true_labels):\n","  all_counts = len(arrivals)\n","  MD = np.count_nonzero(triggers == -1)\n","  FA = np.count_nonzero(trig_labels == -1)\n","\n","  if np.count_nonzero(true_labels == 0) == 0:\n","    n_acc = 100\n","  else:\n","    n_acc = 100*(np.count_nonzero(trig_labels == 0)/np.count_nonzero(true_labels == 0))\n","  if np.count_nonzero(true_labels == 1) == 0:\n","    g_acc = 100\n","  else:\n","    g_acc = 100*(np.count_nonzero(trig_labels == 1)/np.count_nonzero(true_labels == 1))\n","\n","  tp = all_counts - MD\n","\n","  if tp + FA == 0:\n","    precision = 0\n","  else:\n","    precision = tp/(tp+FA)\n","\n","  if tp + MD == 0:\n","    recall = 0\n","  else:\n","    recall = tp/(tp+MD)\n","\n","  if precision + recall == 0:\n","    F1_score = 0\n","  else:\n","    F1_score = 2*precision*recall/(precision + recall)\n","\n","  return all_counts,MD,FA,n_acc,g_acc,precision,recall,F1_score\n","\n","def Deep_Trigger_statistics(N_windows,L_window,neutrons,gammas):\n","\n","  statistics_mat = np.zeros((N_windows,8))\n","\n","  iteration_num = 0\n","  while(True):\n","    perm = np.random.permutation(np.shape(neutrons)[0])\n","    neutrons = neutrons[perm]\n","    gammas = gammas[perm]\n","    noised_data,labels,true_inf,last_arrival_time = generate_pulse_train(neutrons,gammas,lambda_n,\n","                                                                                          lambda_g, return_statistics = False, add_noise = True)\n","    labels_NN = neural_networks_trigger(noised_data, last_arrival_time, prob=True)\n","\n","    triggers_new_out = np.where(labels_NN != 0)[0]\n","    arrivals_out, triggers_out, triggered_events_out, false_triggers_array_out = match_pairs(true_inf[:,0],triggers_new_out,low_NN,high_NN)\n","    triggered_labels = np.where(false_triggers_array_out == -1, -1, labels[false_triggers_array_out.astype(int)] - 1)\n","\n","    true_labels = labels[arrivals_out.astype(int)] - 1\n","\n","    total_curr_length = arrivals_out[len(arrivals_out) - 1] - 2*cont_starting_time\n","    n_iter_curr = int(np.floor(total_curr_length/L_window))\n","    arrivals_idx_lst = [0]\n","    triggers_idx_lst = [0]\n","    curr_idx = 0\n","    for j in range(n_iter_curr):\n","      while(True):\n","        if arrivals_out[curr_idx] >= cont_starting_time + (j+1)*L_window:\n","          arrivals_idx_lst.append(curr_idx)\n","          break\n","        curr_idx += 1\n","    curr_idx = 0\n","    for j in range(n_iter_curr):\n","      while(True):\n","        if triggers_out[curr_idx] >= cont_starting_time + (j+1)*L_window:\n","          triggers_idx_lst.append(curr_idx)\n","          break\n","        curr_idx += 1\n","    for j in range(1,n_iter_curr):\n","\n","      curr_arrivals = arrivals_out[arrivals_idx_lst[j]:arrivals_idx_lst[j + 1]]\n","      curr_triggers = triggered_events_out[arrivals_idx_lst[j]:arrivals_idx_lst[j + 1]]\n","      curr_triggered_labels = triggered_labels[triggers_idx_lst[j]:triggers_idx_lst[j + 1]]\n","      curr_true_labels = true_labels[arrivals_idx_lst[j]:arrivals_idx_lst[j + 1]]\n","      curr_statistics = calc_deep_trigger_window_statistics(curr_arrivals,curr_triggers,curr_triggered_labels,curr_true_labels)\n","      statistics_mat[iteration_num,:] = curr_statistics\n","      iteration_num += 1\n","\n","      if iteration_num == N_windows:\n","        return statistics_mat\n","\n","\n","def create_classifier_data_for_training_from_deep_trigger(neutrons,gammas,N_dataset,extra_feat_state):\n","\n","  if extra_feat_state == True:\n","\n","    discrete_data_out = np.zeros((N_dataset,WL_classifier))\n","    discrete_labels_out = np.zeros(N_dataset)\n","    classification_features_out = np.zeros((N_dataset,num_classification_features))\n","    attention_features_out = np.zeros((N_dataset,num_attention_features))\n","    N_mid = int(N_dataset/2)\n","    curr_n_count = 0\n","    curr_g_count = N_mid\n","\n","    while(True):\n","      perm = np.random.permutation(np.shape(neutrons)[0])\n","      neutrons = neutrons[perm]\n","      gammas = gammas[perm]\n","      noised_data,labels,true_inf,last_arrival_time = generate_pulse_train(neutrons,gammas,lambda_n,\n","                                                                                            lambda_g, return_statistics = False, add_noise = True)\n","\n","      deep_trig_arrival_times = deep_trigger_arrival_times(noised_data, last_arrival_time, prob=True)\n","\n","      arrivals_out, triggers_out, triggered_events_out, false_triggers_array_out = match_pairs(true_inf[:,0],deep_trig_arrival_times,low_NN,high_NN)\n","\n","      triggered_labels = np.where(false_triggers_array_out == -1, -1, labels[false_triggers_array_out.astype(int)] - 1)\n","\n","      discrete_data, discrete_labels, original_idx_lst,classification_features,attention_features = generate_discrete_data(noised_data,triggers_out,triggered_labels,last_arrival_time, extra_features = extra_feat_state, rand_off = False, subtract_baseline = True, WL_classifier = WL_classifier)\n","\n","      for i in range(len(discrete_labels)):\n","        if discrete_labels[i] == 0 and curr_n_count < N_mid:\n","          discrete_data_out[curr_n_count,:] = discrete_data[i,:]\n","          discrete_labels_out[curr_n_count] = discrete_labels[i]\n","          classification_features_out[curr_n_count,:] = classification_features[i,:]\n","          attention_features_out[curr_n_count,:] = attention_features[i,:]\n","          curr_n_count += 1\n","        if discrete_labels[i] == 1 and curr_g_count < N_dataset:\n","          discrete_data_out[curr_g_count,:] = discrete_data[i,:]\n","          discrete_labels_out[curr_g_count] = discrete_labels[i]\n","          classification_features_out[curr_g_count,:] = classification_features[i,:]\n","          attention_features_out[curr_g_count,:] = attention_features[i,:]\n","          curr_g_count += 1\n","        if curr_n_count == N_mid and curr_g_count == N_dataset:\n","          perm = np.random.permutation(np.shape(discrete_data_out)[0])\n","          discrete_data_out = discrete_data_out[perm]\n","          discrete_labels_out = discrete_labels_out[perm]\n","          classification_features_out = classification_features_out[perm]\n","          attention_features_out = attention_features_out[perm]\n","\n","          return discrete_data_out, discrete_labels_out, classification_features_out, attention_features_out\n","\n","  else:\n","\n","    discrete_data_out = np.zeros((N_dataset,WL_classifier))\n","    discrete_labels_out = np.zeros(N_dataset)\n","    N_mid = int(N_dataset/2)\n","    curr_n_count = 0\n","    curr_g_count = N_mid\n","\n","    while(True):\n","      perm = np.random.permutation(np.shape(neutrons)[0])\n","      neutrons = neutrons[perm]\n","      gammas = gammas[perm]\n","      noised_data,labels,true_inf,last_arrival_time = generate_pulse_train(neutrons,gammas,lambda_n,\n","                                                                                            lambda_g, return_statistics = False, add_noise = True)\n","      deep_trig_arrival_times = deep_trigger_arrival_times(noised_data, last_arrival_time, prob=True)\n","\n","      arrivals_out, triggers_out, triggered_events_out, false_triggers_array_out = match_pairs(true_inf[:,0],deep_trig_arrival_times,low_NN,high_NN)\n","\n","      triggered_labels = np.where(false_triggers_array_out == -1, -1, labels[false_triggers_array_out.astype(int)] - 1)\n","\n","      discrete_data, discrete_labels, original_idx_lst = generate_discrete_data(noised_data,triggers_out,triggered_labels,last_arrival_time, extra_features = extra_feat_state, rand_off = False, subtract_baseline = True, WL_classifier = WL_classifier)\n","\n","      for i in range(len(discrete_labels)):\n","        if discrete_labels[i] == 0 and curr_n_count < N_mid:\n","          discrete_data_out[curr_n_count,:] = discrete_data[i,:]\n","          discrete_labels_out[curr_n_count] = discrete_labels[i]\n","          curr_n_count += 1\n","        if discrete_labels[i] == 1 and curr_g_count < N_dataset:\n","          discrete_data_out[curr_g_count,:] = discrete_data[i,:]\n","          discrete_labels_out[curr_g_count] = discrete_labels[i]\n","          curr_g_count += 1\n","        if curr_n_count == N_mid and curr_g_count == N_dataset:\n","          perm = np.random.permutation(np.shape(discrete_data_out)[0])\n","          discrete_data_out = discrete_data_out[perm]\n","          discrete_labels_out = discrete_labels_out[perm]\n","\n","          return discrete_data_out, discrete_labels_out\n","\n","def create_classifier_data_for_training(neutrons,gammas,N_dataset,extra_feat_state):\n","\n","  if extra_feat_state == True:\n","\n","    discrete_data_out = np.zeros((N_dataset,WL_classifier))\n","    discrete_labels_out = np.zeros(N_dataset)\n","    classification_features_out = np.zeros((N_dataset,num_classification_features))\n","    attention_features_out = np.zeros((N_dataset,num_attention_features))\n","    N_mid = int(N_dataset/2)\n","    curr_n_count = 0\n","    curr_g_count = N_mid\n","\n","    while(True):\n","      perm = np.random.permutation(np.shape(neutrons)[0])\n","      neutrons = neutrons[perm]\n","      gammas = gammas[perm]\n","      noised_data,labels,true_inf,last_arrival_time = generate_pulse_train(neutrons,gammas,lambda_n,\n","                                                                                            lambda_g, return_statistics = False, add_noise = True)\n","\n","      discrete_data, discrete_labels, original_idx_lst,classification_features,attention_features = generate_discrete_data(noised_data,true_inf[:,0],true_inf[:,1],last_arrival_time, extra_features = extra_feat_state, rand_off = False, subtract_baseline = True, WL_classifier = WL_classifier)\n","\n","      for i in range(len(discrete_labels)):\n","        if discrete_labels[i] == 0 and curr_n_count < N_mid:\n","          discrete_data_out[curr_n_count,:] = discrete_data[i,:]\n","          discrete_labels_out[curr_n_count] = discrete_labels[i]\n","          classification_features_out[curr_n_count,:] = classification_features[i,:]\n","          attention_features_out[curr_n_count,:] = attention_features[i,:]\n","          curr_n_count += 1\n","        if discrete_labels[i] == 1 and curr_g_count < N_dataset:\n","          discrete_data_out[curr_g_count,:] = discrete_data[i,:]\n","          discrete_labels_out[curr_g_count] = discrete_labels[i]\n","          classification_features_out[curr_g_count,:] = classification_features[i,:]\n","          attention_features_out[curr_g_count,:] = attention_features[i,:]\n","          curr_g_count += 1\n","        if curr_n_count == N_mid and curr_g_count == N_dataset:\n","          perm = np.random.permutation(np.shape(discrete_data_out)[0])\n","          discrete_data_out = discrete_data_out[perm]\n","          discrete_labels_out = discrete_labels_out[perm]\n","          classification_features_out = classification_features_out[perm]\n","          attention_features_out = attention_features_out[perm]\n","\n","          return discrete_data_out, discrete_labels_out, classification_features_out, attention_features_out\n","\n","  else:\n","\n","    discrete_data_out = np.zeros((N_dataset,WL_classifier))\n","    discrete_labels_out = np.zeros(N_dataset)\n","    N_mid = int(N_dataset/2)\n","    curr_n_count = 0\n","    curr_g_count = N_mid\n","\n","    while(True):\n","      perm = np.random.permutation(np.shape(neutrons)[0])\n","      neutrons = neutrons[perm]\n","      gammas = gammas[perm]\n","      noised_data,labels,true_inf,last_arrival_time = generate_pulse_train(neutrons,gammas,lambda_n,\n","                                                                                            lambda_g, return_statistics = False, add_noise = True)\n","\n","      discrete_data, discrete_labels, original_idx_lst = generate_discrete_data(noised_data,true_inf[:,0],true_inf[:,1],last_arrival_time, extra_features = extra_feat_state, rand_off = False, subtract_baseline = True, WL_classifier = WL_classifier)\n","\n","      for i in range(len(discrete_labels)):\n","        if discrete_labels[i] == 0 and curr_n_count < N_mid:\n","          discrete_data_out[curr_n_count,:] = discrete_data[i,:]\n","          discrete_labels_out[curr_n_count] = discrete_labels[i]\n","          curr_n_count += 1\n","        if discrete_labels[i] == 1 and curr_g_count < N_dataset:\n","          discrete_data_out[curr_g_count,:] = discrete_data[i,:]\n","          discrete_labels_out[curr_g_count] = discrete_labels[i]\n","          curr_g_count += 1\n","        if curr_n_count == N_mid and curr_g_count == N_dataset:\n","          perm = np.random.permutation(np.shape(discrete_data_out)[0])\n","          discrete_data_out = discrete_data_out[perm]\n","          discrete_labels_out = discrete_labels_out[perm]\n","\n","          return discrete_data_out, discrete_labels_out\n"],"metadata":{"id":"nE3JZSq8yFRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def classifiers_statistics_with_deep_trigger(N_windows,L_window,neutrons,gammas):\n","\n","  statistics_mat = np.zeros((N_windows,8))\n","  statistics_mat_CI = np.zeros((N_windows,4))\n","  statistics_mat_corr = np.zeros((N_windows,4))\n","  statistics_mat_FC_NN = np.zeros((N_windows,4))\n","  statistics_mat_concat = np.zeros((N_windows,4))\n","  statistics_mat_attention = np.zeros((N_windows,4))\n","\n","  iteration_num = 0\n","  while(True):\n","    perm = np.random.permutation(np.shape(neutrons)[0])\n","    neutrons = neutrons[perm]\n","    gammas = gammas[perm]\n","    noised_data,labels,true_inf,last_arrival_time = generate_pulse_train(neutrons,gammas,lambda_n,\n","                                                                                          lambda_g, return_statistics = False, add_noise = True)\n","\n","\n","    labels_NN = neural_networks_trigger(noised_data, last_arrival_time, prob=True)\n","\n","    triggers_new_out = []\n","    for s in range(len(labels_NN)):\n","      if labels_NN[s] != 0:\n","        triggers_new_out.append(s)\n","\n","    arrivals_out, triggers_out, triggered_events_out, false_triggers_array_out = match_pairs(true_inf[:,0],triggers_new_out,low_NN,high_NN)\n","\n","    triggered_labels = np.zeros(len(triggers_out))\n","    for s in range(len(triggers_out)):\n","      if false_triggers_array_out[s] == -1:\n","        triggered_labels[s] = -1\n","      else:\n","        triggered_labels[s] = labels[int(false_triggers_array_out[s])] - 1\n","\n","    true_labels = np.zeros(len(arrivals_out))\n","    for s in range(len(arrivals_out)):\n","      true_labels[s] = labels[int(arrivals_out[s])] - 1\n","\n","\n","    discrete_data, discrete_labels, original_idx_lst,classification_features,attention_features = generate_discrete_data_with_FA(noised_data,triggers_out,triggered_labels,last_arrival_time, extra_features = True, rand_off = False, subtract_baseline = True, WL_classifier = WL_classifier)\n","\n","    CI_labels, CI_values = charge_integration(discrete_data,CI_parameters)\n","    corr_labels, corr_values = correlation_classifier(discrete_data,Corr_parameters)\n","    FC_NN_labels, FC_NN_values = create_labels_Model_FC_NN(model_fc_nn,discrete_data,norm_params_model_fc_nn)\n","    concat_labels, concat_values = create_labels_Model_Concat(model_concat,discrete_data,classification_features,attention_features,norm_params_model_concat)\n","    attention_labels, attention_values = create_labels_Model_Attention(model_attention,discrete_data,classification_features,attention_features,norm_params_model_attention)\n","\n","    total_curr_length = last_arrival_time - 2*cont_starting_time\n","    n_iter_curr = int(np.floor(total_curr_length/L_window))\n","\n","    if triggers_out[0] < cont_starting_time:\n","      zero_idx = 0\n","      while(True):\n","        if triggers_out[zero_idx] >= cont_starting_time:\n","          break\n","        zero_idx += 1\n","\n","      triggers_out = triggers_out[zero_idx:]\n","      triggered_labels = triggered_labels[zero_idx:]\n","\n","    arrivals_idx_lst = [0]\n","    triggers_idx_lst = [0]\n","    curr_idx = 0\n","    for j in range(n_iter_curr):\n","      while(True):\n","        if arrivals_out[curr_idx] >= cont_starting_time + (j+1)*L_window:\n","          arrivals_idx_lst.append(curr_idx)\n","          break\n","        curr_idx += 1\n","    curr_idx = 0\n","    for j in range(n_iter_curr):\n","      while(True):\n","        if triggers_out[curr_idx] >= cont_starting_time + (j+1)*L_window:\n","          triggers_idx_lst.append(curr_idx)\n","          break\n","        curr_idx += 1\n","\n","\n","    for j in range(1,n_iter_curr):\n","\n","      curr_arrivals = arrivals_out[arrivals_idx_lst[j]:arrivals_idx_lst[j + 1]]\n","      curr_triggers = triggered_events_out[arrivals_idx_lst[j]:arrivals_idx_lst[j + 1]]\n","      curr_triggered_labels = discrete_labels[triggers_idx_lst[j]:triggers_idx_lst[j + 1]]\n","      curr_true_labels = true_labels[arrivals_idx_lst[j]:arrivals_idx_lst[j + 1]]\n","\n","      statistics_mat[iteration_num,:] = calc_deep_trigger_window_statistics(curr_arrivals,curr_triggers,curr_triggered_labels,curr_true_labels)\n","\n","      curr_CI_labels = CI_labels[triggers_idx_lst[j]:triggers_idx_lst[j + 1]]\n","      curr_corr_labels = corr_labels[triggers_idx_lst[j]:triggers_idx_lst[j + 1]]\n","      curr_FC_NN_labels = FC_NN_labels[triggers_idx_lst[j]:triggers_idx_lst[j + 1]]\n","      curr_concat_labels = concat_labels[triggers_idx_lst[j]:triggers_idx_lst[j + 1]]\n","      curr_attention_labels = attention_labels[triggers_idx_lst[j]:triggers_idx_lst[j + 1]]\n","\n","      statistics_mat_CI[iteration_num,:] = calc_classifiers_and_deep_trigger_window_statistics(curr_CI_labels,curr_triggered_labels)\n","      statistics_mat_corr[iteration_num,:] = calc_classifiers_and_deep_trigger_window_statistics(curr_corr_labels,curr_triggered_labels)\n","      statistics_mat_FC_NN[iteration_num,:] = calc_classifiers_and_deep_trigger_window_statistics(curr_FC_NN_labels,curr_triggered_labels)\n","      statistics_mat_concat[iteration_num,:] = calc_classifiers_and_deep_trigger_window_statistics(curr_concat_labels,curr_triggered_labels)\n","      statistics_mat_attention[iteration_num,:] = calc_classifiers_and_deep_trigger_window_statistics(curr_attention_labels,curr_triggered_labels)\n","\n","      iteration_num += 1\n","\n","      if iteration_num == N_windows:\n","        return statistics_mat,statistics_mat_CI,statistics_mat_corr,statistics_mat_FC_NN,statistics_mat_concat,statistics_mat_attention\n","\n","\n","def calc_classifiers_and_deep_trigger_window_statistics(method_labels,true_labels):\n","\n","  all_counts = np.count_nonzero(true_labels != -1)\n","\n","  if all_counts == 0:\n","    acc = 100\n","  else:\n","    acc = 100*(np.count_nonzero(method_labels == true_labels)/all_counts)\n","\n","  num_neutrons = np.count_nonzero(true_labels == 0)\n","  num_gammas = np.count_nonzero(true_labels == 1)\n","\n","  num_correct_neutrons = np.count_nonzero(np.logical_and(true_labels == 0, true_labels == method_labels))\n","  num_correct_gammas = np.count_nonzero(np.logical_and(true_labels == 1, true_labels == method_labels))\n","\n","  if num_neutrons == 0:\n","    neutron_acc = 100\n","  else:\n","    neutron_acc = 100*(num_correct_neutrons/num_neutrons)\n","\n","  if num_gammas == 0:\n","    gamma_acc = 100\n","  else:\n","    gamma_acc = 100*(num_correct_gammas/num_gammas)\n","\n","  return all_counts, acc, neutron_acc, gamma_acc"],"metadata":{"id":"Gw7jiQkNufSP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def classifiers_statistics_known_arrivals(N_windows,L_window,neutrons,gammas):\n","\n","  statistics_mat_CI = np.zeros((N_windows,4))\n","  statistics_mat_corr = np.zeros((N_windows,4))\n","  statistics_mat_FC_NN = np.zeros((N_windows,4))\n","  statistics_mat_concat = np.zeros((N_windows,4))\n","  statistics_mat_attention = np.zeros((N_windows,4))\n","\n","  iteration_num = 0\n","  while(True):\n","    perm = np.random.permutation(np.shape(neutrons)[0])\n","    neutrons = neutrons[perm]\n","    gammas = gammas[perm]\n","    noised_data,labels,true_inf,last_arrival_time = generate_pulse_train(neutrons,gammas,lambda_n,\n","                                                                                          lambda_g, return_statistics = False, add_noise = True)\n","\n","    discrete_data, discrete_labels, original_idx_lst,classification_features,attention_features = generate_discrete_data(noised_data,true_inf[:,0],true_inf[:,1],last_arrival_time, extra_features = True, rand_off = False, subtract_baseline = True, WL_classifier = WL_classifier)\n","\n","    CI_labels, CI_values = charge_integration(discrete_data,CI_parameters)\n","    corr_labels, corr_values = correlation_classifier(discrete_data,Corr_parameters)\n","    FC_NN_labels, FC_NN_values = create_labels_Model_FC_NN(model_fc_nn,discrete_data,norm_params_model_fc_nn)\n","    concat_labels, concat_values = create_labels_Model_Concat(model_concat,discrete_data,classification_features,attention_features,norm_params_model_concat)\n","    attention_labels, attention_values = create_labels_Model_Attention(model_attention,discrete_data,classification_features,attention_features,norm_params_model_attention)\n","\n","    now_arr_times = true_inf[:,0]\n","\n","    total_curr_length = last_arrival_time - 2*cont_starting_time\n","    n_iter_curr = int(np.floor(total_curr_length/L_window))\n","\n","    zero_idx = 0\n","    while(True):\n","      if now_arr_times[zero_idx] >= cont_starting_time:\n","        break\n","      zero_idx += 1\n","\n","    updated_now_arr_times = now_arr_times[zero_idx:]\n","    arrivals_idx_lst = [0]\n","    curr_idx = 0\n","    for j in range(n_iter_curr):\n","      while(True):\n","        if updated_now_arr_times[curr_idx] >= cont_starting_time + (j+1)*L_window:\n","          arrivals_idx_lst.append(curr_idx)\n","          break\n","        curr_idx += 1\n","\n","    for j in range(1,n_iter_curr):\n","\n","          curr_CI_labels = CI_labels[arrivals_idx_lst[j]:arrivals_idx_lst[j + 1]]\n","          curr_corr_labels = corr_labels[arrivals_idx_lst[j]:arrivals_idx_lst[j + 1]]\n","          curr_FC_NN_labels = FC_NN_labels[arrivals_idx_lst[j]:arrivals_idx_lst[j + 1]]\n","          curr_concat_labels = concat_labels[arrivals_idx_lst[j]:arrivals_idx_lst[j + 1]]\n","          curr_attention_labels = attention_labels[arrivals_idx_lst[j]:arrivals_idx_lst[j + 1]]\n","          curr_true_labels = discrete_labels[arrivals_idx_lst[j]:arrivals_idx_lst[j + 1]]\n","\n","          statistics_mat_CI[iteration_num,:] = calc_classifiers_known_arrivals_window_statistics(curr_CI_labels,curr_true_labels)\n","          statistics_mat_corr[iteration_num,:] = calc_classifiers_known_arrivals_window_statistics(curr_corr_labels,curr_true_labels)\n","          statistics_mat_FC_NN[iteration_num,:] = calc_classifiers_known_arrivals_window_statistics(curr_FC_NN_labels,curr_true_labels)\n","          statistics_mat_concat[iteration_num,:] = calc_classifiers_known_arrivals_window_statistics(curr_concat_labels,curr_true_labels)\n","          statistics_mat_attention[iteration_num,:] = calc_classifiers_known_arrivals_window_statistics(curr_attention_labels,curr_true_labels)\n","          iteration_num += 1\n","\n","          if iteration_num == N_windows:\n","            return statistics_mat_CI,statistics_mat_corr,statistics_mat_FC_NN,statistics_mat_concat,statistics_mat_attention\n","\n","def calc_classifiers_known_arrivals_window_statistics(method_labels,true_labels):\n","\n","  all_counts = len(true_labels)\n","  if all_counts == 0:\n","    acc = 100\n","  else:\n","    acc = 100*(np.count_nonzero(method_labels == true_labels)/all_counts)\n","\n","  num_neutrons = np.count_nonzero(true_labels == 0)\n","  num_gammas = np.count_nonzero(true_labels == 1)\n","\n","  num_correct_neutrons = np.count_nonzero(np.logical_and(true_labels == 0, true_labels == method_labels))\n","  num_correct_gammas = np.count_nonzero(np.logical_and(true_labels == 1, true_labels == method_labels))\n","\n","  if num_neutrons == 0:\n","    neutron_acc = 100\n","  else:\n","    neutron_acc = 100*(num_correct_neutrons/num_neutrons)\n","\n","  if num_gammas == 0:\n","    gamma_acc = 100\n","  else:\n","    gamma_acc = 100*(num_correct_gammas/num_gammas)\n","\n","  return all_counts, acc, neutron_acc, gamma_acc"],"metadata":{"id":"h4t5wQjYnRoF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_SDL_signal(data, SDL_delay, SDL_coeff):\n","    SDL_delay = int(SDL_delay)\n","    SDL_coeff = int(SDL_coeff)\n","\n","    delayed_data = data[SDL_delay:]\n","    scaled_data = SDL_coeff * data[:len(data) - SDL_delay]\n","\n","    if SDL_delay != 0:\n","      SDL_signal = np.concatenate((np.zeros(SDL_delay),delayed_data - scaled_data))\n","    else:\n","      SDL_signal = delayed_data - scaled_data\n","\n","    return SDL_signal\n","\n","def create_SDL_THR_labels(data,end,thr,jump_deriv):\n","\n","  jump_deriv = int(jump_deriv)\n","  labels_out = np.zeros(len(data))\n","  i = cont_starting_time\n","\n","  while(i  < end):\n","    if thr > data[i-1]  and data[i] > thr:\n","      labels_out[i] = 1\n","      i += jump_deriv\n","    i+=1\n","\n","  return labels_out\n","\n","def generate_correlation_data(data, template, temp_start, temp_end):\n","    temp_start = int(temp_start)\n","    temp_end = int(temp_end)\n","    current_template = template[temp_start:temp_end]\n","    sigma_y = np.std(current_template)\n","    mean_y = np.mean(current_template)\n","    N = len(current_template)\n","    corr_data_out = np.zeros(len(data))\n","\n","    x_slices = np.lib.stride_tricks.sliding_window_view(data, window_shape=(temp_end - temp_start,))\n","    sigma_x = np.std(x_slices, axis=1)\n","    mean_x = np.mean(x_slices, axis=1)\n","\n","    normalized_x = (x_slices - mean_x[:, np.newaxis]) / sigma_x[:, np.newaxis]\n","    normalized_template = (current_template - mean_y) / sigma_y\n","\n","    corr_data = np.sum(normalized_x * normalized_template, axis=1) / N\n","    corr_data_out[:len(corr_data)] = corr_data\n","\n","    return corr_data_out\n","\n","def generate_correlation_data_old(data, template, temp_start, temp_end):\n","    temp_start = int(temp_start)\n","    temp_end = int(temp_end)\n","    current_template = template[temp_start:temp_end]\n","    sigma_y = np.std(current_template)\n","    mean_y = np.mean(current_template)\n","    N = len(current_template)\n","    corr_data_out = np.zeros(len(data))\n","    normalized_template = (current_template - mean_y) / sigma_y\n","\n","    jumpy = 100000\n","    j = 0\n","    for i in range(0,len(data) - (temp_end - temp_start) - jumpy,jumpy):\n","\n","      x_slices = np.lib.stride_tricks.sliding_window_view(data[i:i + jumpy + (temp_end - temp_start) + 5], window_shape=(temp_end - temp_start,))\n","      sigma_x = np.std(x_slices, axis=1)\n","      mean_x = np.mean(x_slices, axis=1)\n","      normalized_x = (x_slices - mean_x[:, np.newaxis]) / sigma_x[:, np.newaxis]\n","      corr_data = np.sum(normalized_x * normalized_template, axis=1) / N\n","      corr_data_out[i:i + len(corr_data)] = corr_data\n","      j = i + len(corr_data)\n","\n","    if j < (len(data) - temp_end):\n","      for i in range(j,(len(data) - temp_end)):\n","        x = data[i + temp_start:i+temp_end]\n","        sigma_x = np.std(x)\n","        mean_x = np.mean(x)\n","        corr_data_out[i] = np.dot((x - mean_x), (current_template - mean_y)) / (N * sigma_x * sigma_y)\n","\n","    return corr_data_out\n","\n","def generate_correlation_data_old1(data, template, temp_start, temp_end):\n","    temp_start = int(temp_start)\n","    temp_end = int(temp_end)\n","    current_template = template[temp_start:temp_end]\n","    sigma_y = np.std(current_template)\n","    mean_y = np.mean(current_template)\n","    N = len(current_template)\n","    corr_data = np.zeros(len(data))\n","\n","    for i in range(len(data) - temp_end):\n","      x = data[i + temp_start:i+temp_end]\n","      sigma_x = np.std(x)\n","      mean_x = np.mean(x)\n","      corr_data[i] = np.dot((x - mean_x), (current_template - mean_y)) / (N * sigma_x * sigma_y)\n","\n","    return corr_data\n","\n","def create_Corr_THR_labels(data1,data2,end,thr,jump_deriv):\n","\n","  jump_deriv = int(jump_deriv)\n","  labels_out = np.zeros(len(data1))\n","  i = cont_starting_time\n","\n","  while(i  < end):\n","    if (thr > data1[i-1]  and data1[i] > thr) or (thr > data2[i-1]  and data2[i] > thr):\n","      labels_out[i] = 1\n","      i += jump_deriv\n","    i+=1\n","\n","  return labels_out"],"metadata":{"id":"b24qA2m7n249"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Deep_Trigger_Only_Results(result_trig_iters,neutrons,gammas):\n","\n","  statistics_mat = np.zeros((3,8))\n","\n","  for j in range(result_trig_iters):\n","    perm = np.random.permutation(np.shape(neutrons)[0])\n","    neutrons = neutrons[perm]\n","    gammas = gammas[perm]\n","    noised_data,labels,true_inf,last_arrival_time = generate_pulse_train(neutrons,gammas,lambda_n,\n","                                                                                          lambda_g, return_statistics = False, add_noise = True)\n","\n","    deep_trig_arrival_times = deep_trigger_arrival_times(noised_data, last_arrival_time, prob=True)\n","    deep_trig_arrival_times_NM = deep_trigger_arrival_times_NM(noised_data, last_arrival_time, prob=True)\n","    corr_arr_times = corr_trig_arrival_times(noised_data,last_arrival_time,BM_corr_params)\n","\n","    statistics_mat[0,:] += Deep_Trigger_statistics_results(deep_trig_arrival_times,labels,true_inf,low_NN,high_NN)\n","    statistics_mat[1,:] += Deep_Trigger_statistics_results(deep_trig_arrival_times_NM,labels,true_inf,low_NN,high_NN)\n","    statistics_mat[2,:] += Deep_Trigger_statistics_results(corr_arr_times,labels,true_inf,BM_corr_params[4],(low_NN + high_NN) - BM_corr_params[4])\n","\n","  return statistics_mat/result_trig_iters"],"metadata":{"id":"HbTZTOgTWHJ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Deep_Trigger_All_BM_Results(result_trig_iters,neutrons,gammas):\n","\n","  statistics_mat = np.zeros((7,8))\n","\n","  for j in range(result_trig_iters):\n","    perm = np.random.permutation(np.shape(neutrons)[0])\n","    neutrons = neutrons[perm]\n","    gammas = gammas[perm]\n","    noised_data,labels,true_inf,cont_peak_labels,true_inf_peaks,last_arrival_time = generate_pulse_train_peak_labels(neutrons,gammas,lambda_n,\n","                                                                                          lambda_g, add_noise = True)\n","\n","    deep_trig_arrival_times = deep_trigger_arrival_times(noised_data, last_arrival_time, prob=True)\n","    BM1_arr_times = BM1_arrival_times(noised_data,last_arrival_time,BM1_params)\n","    BM2_arr_times = BM2_arrival_times(noised_data,last_arrival_time,BM2_params)\n","    corr_n_trig_arr_times = corr_n_trig_arrival_times(noised_data,last_arrival_time,corr_n_trig_params)\n","    corr_g_trig_arr_times = corr_g_trig_arrival_times(noised_data,last_arrival_time,corr_g_trig_params)\n","    SDL_normal_THR_arr_times = SDL_normal_THR_arrival_times(noised_data,last_arrival_time,SDL_normal_THR_params)\n","    SDL_find_peaks_arr_times = SDL_find_peaks_arrival_times(noised_data,last_arrival_time,SDL_find_peaks_params)\n","\n","    statistics_mat[0,:] += Deep_Trigger_statistics_results(deep_trig_arrival_times,labels,true_inf,low_NN,high_NN)\n","    statistics_mat[1,:] += Deep_Trigger_statistics_results(BM1_arr_times,labels,true_inf,BM1_params[5],(low_NN + high_NN) - BM2_params[5])\n","    statistics_mat[2,:] += Deep_Trigger_statistics_results(BM2_arr_times,labels,true_inf,BM2_params[2],(low_NN + high_NN) - BM2_params[2])\n","    statistics_mat[3,:] += Deep_Trigger_statistics_results(corr_n_trig_arr_times,labels,true_inf,corr_n_trig_params[4],(low_NN + high_NN) - corr_n_trig_params[4])\n","    statistics_mat[4,:] += Deep_Trigger_statistics_results(corr_g_trig_arr_times,labels,true_inf,corr_g_trig_params[4],(low_NN + high_NN) - corr_g_trig_params[4])\n","    statistics_mat[5,:] += Deep_Trigger_statistics_results(SDL_normal_THR_arr_times,labels,true_inf,SDL_normal_THR_params[4],(low_NN + high_NN) - SDL_normal_THR_params[4])\n","    statistics_mat[6,:] += Deep_Trigger_statistics_results(SDL_find_peaks_arr_times,cont_peak_labels,true_inf_peaks,low_NN,high_NN)\n","\n","  return statistics_mat/result_trig_iters\n","\n","def BM1_arrival_times(data_in,end,params):\n","\n","  cont_deriv_sig = create_SDL_signal(data_in,params[6],1)\n","  potential_labels = create_SDL_THR_labels(cont_deriv_sig,end,params[0],params[4])\n","  pot_labels_curr = np.where((potential_labels != 0))[0]\n","  arrival_times = []\n","  for idx in pot_labels_curr:\n","    if np.max(data_in[idx + params[1]:idx + params[2]]) - np.mean(data_in[idx + params[1]:idx + params[2]]) > params[3]:\n","      arrival_times.append(idx)\n","\n","  return np.array(arrival_times)\n","\n","def BM2_arrival_times(data_in,end,params):\n","\n","  cont_deriv_sig = create_SDL_signal(data_in,params[3],1)\n","  SDL_labels = create_SDL_THR_labels(cont_deriv_sig,end,params[0],params[1])\n","  arrival_times = np.where(SDL_labels != 0)[0]\n","\n","  return arrival_times\n","\n","def corr_trig_arrival_times(data_in,end,params):\n","\n","  cont_corr_sig1 = generate_correlation_data_old(data_in,template_n,params[0],params[1])\n","  cont_corr_sig2 = generate_correlation_data_old(data_in,template_g,params[0],params[1])\n","  corr_labels = create_Corr_THR_labels(cont_corr_sig1,cont_corr_sig2,end,params[2],params[3])\n","  arrival_times = np.where(corr_labels != 0)[0]\n","\n","  return arrival_times\n","\n","def corr_n_trig_arrival_times(data_in,end,params):\n","\n","  cont_corr_sig = generate_correlation_data_old(data_in,template_n,params[0],params[1])\n","  corr_labels = create_SDL_THR_labels(cont_corr_sig,end,params[2],params[3])\n","  arrival_times = np.where(corr_labels != 0)[0]\n","\n","  return arrival_times\n","\n","def corr_g_trig_arrival_times(data_in,end,params):\n","\n","  cont_corr_sig = generate_correlation_data_old(data_in,template_g,params[0],params[1])\n","  corr_labels = create_SDL_THR_labels(cont_corr_sig,end,params[2],params[3])\n","  arrival_times = np.where(corr_labels != 0)[0]\n","\n","  return arrival_times\n","\n","\n","def SDL_normal_THR_arrival_times(data_in,end,params):\n","\n","  cont_deriv_sig = create_SDL_signal(data_in,params[1],params[2])\n","  SDL_labels = create_SDL_THR_labels(cont_deriv_sig,end,params[0],params[3])\n","  arrival_times = np.where(SDL_labels != 0)[0]\n","\n","  return arrival_times\n","\n","def SDL_find_peaks_arrival_times(data_in,end,params):\n","\n","  cont_deriv_sig = create_SDL_signal(data_in,params[0],params[1])\n","  arrival_times = find_peaks(cont_deriv_sig,height = 0.017, distance = params[2], prominence=params[3], width=params[4])\n","\n","  return arrival_times\n","\n","def Deep_Trigger_statistics_results(trig_arrival_times,labels,true_inf,curr_low,curr_high):\n","\n","  arrivals_out, triggers_out, triggered_events_out, false_triggers_array_out = match_pairs(true_inf[:,0],trig_arrival_times,curr_low,curr_high)\n","  triggered_labels = np.where(false_triggers_array_out == -1, -1, labels[false_triggers_array_out.astype(int)] - 1)\n","  true_labels = labels[arrivals_out.astype(int)] - 1\n","\n","  st_idx = max(arrivals_out[1],triggers_out[1])\n","  en_idx = min(arrivals_out[len(arrivals_out)-1],triggers_out[len(triggers_out)-1])\n","\n","  st1 = np.argmax(arrivals_out >= st_idx)\n","  st2 = np.argmax(triggers_out >= st_idx)\n","  en1 = len(arrivals_out) - np.argmax(arrivals_out[::-1] <= en_idx) - 1\n","  en2 = len(triggers_out) - np.argmax(triggers_out[::-1] <= en_idx) - 1\n","\n","  arrivals_out = arrivals_out[st1:en1]\n","  triggered_events_out = triggered_events_out[st1:en1]\n","  true_labels = true_labels[st1:en1]\n","  triggers_out = triggers_out[st2:en2]\n","  triggered_labels = triggered_labels[st2:en2]\n","\n","  curr_statistics = calc_deep_trigger_window_statistics(arrivals_out,triggered_events_out,triggered_labels,true_labels)\n","\n","  return curr_statistics"],"metadata":{"id":"X6JRfZDRDaGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Classifier_Known_Arrivals_All_BM_Results(result_trig_iters,neutrons,gammas):\n","\n","  statistics_mat = np.zeros((7,10))\n","\n","  for j in range(result_trig_iters):\n","    perm = np.random.permutation(np.shape(neutrons)[0])\n","    neutrons = neutrons[perm]\n","    gammas = gammas[perm]\n","    noised_data,labels,true_inf,last_arrival_time = generate_pulse_train(neutrons,gammas,lambda_n,\n","                                                                                          lambda_g, return_statistics = False, add_noise = True)\n","\n","    discrete_data, discrete_labels, original_idx_lst,classification_features,attention_features = generate_discrete_data(noised_data,true_inf[:,0],true_inf[:,1],last_arrival_time, extra_features = True, rand_off = False, subtract_baseline = True, WL_classifier = WL_classifier)\n","\n","\n","    CI_labels, CI_values = charge_integration(discrete_data,CI_parameters)\n","    corr_labels, corr_values = correlation_classifier(discrete_data,Corr_parameters)\n","    FC_NN_labels, FC_NN_values = create_labels_Model_FC_NN(model_fc_nn,discrete_data,norm_params_model_fc_nn)\n","    concat_labels, concat_values = create_labels_Model_Concatenate(model_concat,discrete_data,classification_features,attention_features,norm_params_model_concat)\n","    attention_labels, attention_values = create_labels_Model_Attention(model_attention,discrete_data,classification_features,attention_features,norm_params_model_attention)\n","\n","    discrete_data1, discrete_labels1, original_idx_lst1 = generate_discrete_data_BM_NN(noised_data,true_inf[:,0],true_inf[:,1],last_arrival_time, subtract_baseline = True, WL_classifier = WL_classifier)\n","\n","    BM_NN_acc = test_final_Model_BM_NN(model_bm_nn,discrete_data1,norm_params_model_bm_nn,discrete_labels1)\n","    BM_NN12_acc = test_final_Model_BM_NN12(discrete_data1,discrete_labels1,model_bm_nn1,norm_params_model_bm_nn1,model_bm_nn2,norm_params_model_bm_nn2)\n","\n","    statistics_mat[0,:] += Classifiers_statistics_results_all_BM(CI_labels,discrete_labels)\n","    statistics_mat[1,:] += Classifiers_statistics_results_all_BM(corr_labels,discrete_labels)\n","    statistics_mat[2,:] += Classifiers_statistics_results_all_BM(FC_NN_labels,discrete_labels)\n","    statistics_mat[3,:] += Classifiers_statistics_results_all_BM(concat_labels,discrete_labels)\n","    statistics_mat[4,:] += Classifiers_statistics_results_all_BM(attention_labels,discrete_labels)\n","    statistics_mat[5,3] += BM_NN_acc\n","    statistics_mat[6,3] += BM_NN12_acc\n","\n","  return statistics_mat/result_trig_iters\n","\n","def Classifiers_statistics_results_all_BM(method_labels,true_labels):\n","\n","  all_counts = len(true_labels)\n","  if all_counts == 0:\n","    acc = 100\n","  else:\n","    acc = 100*(np.count_nonzero(method_labels == true_labels)/all_counts)\n","\n","  num_neutrons = np.count_nonzero(true_labels == 0)\n","  num_gammas = np.count_nonzero(true_labels == 1)\n","\n","  num_classified_neutrons = np.count_nonzero(method_labels == 0)\n","  num_classified_gammas = np.count_nonzero(method_labels == 1)\n","\n","  error_overall_neutrons = 100*np.abs((num_neutrons - num_classified_neutrons)/num_neutrons)\n","  error_overall_gammas = 100*np.abs((num_gammas - num_classified_gammas)/num_gammas)\n","\n","  num_correct_neutrons = np.count_nonzero(np.logical_and(true_labels == 0, true_labels == method_labels))\n","  num_correct_gammas = np.count_nonzero(np.logical_and(true_labels == 1, true_labels == method_labels))\n","\n","  if num_neutrons == 0:\n","    neutron_acc = 100\n","  else:\n","    neutron_acc = 100*(num_correct_neutrons/num_neutrons)\n","\n","  if num_gammas == 0:\n","    gamma_acc = 100\n","  else:\n","    gamma_acc = 100*(num_correct_gammas/num_gammas)\n","\n","  return all_counts,num_neutrons,num_gammas, acc, neutron_acc, gamma_acc, num_classified_neutrons, num_classified_gammas, error_overall_neutrons, error_overall_gammas"],"metadata":{"id":"gRjmqnvKADtI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Combined_Classifier_statistics_results(method_labels,true_labels):\n","\n","  num_elements = len(true_labels)\n","  num_false_alarms = np.count_nonzero(true_labels == -1)\n","  num_true_detections = num_elements - num_false_alarms\n","\n","  if num_true_detections == 0:\n","    acc = 100\n","  else:\n","    acc = 100*(np.count_nonzero(method_labels == true_labels)/num_true_detections)\n","\n","  num_neutrons = np.count_nonzero(true_labels == 0)\n","  num_gammas = np.count_nonzero(true_labels == 1)\n","\n","\n","  num_classified_neutrons = np.count_nonzero(method_labels == 0)\n","  num_classified_gammas = np.count_nonzero(method_labels == 1)\n","\n","  error_overall_neutrons = 100*np.abs((num_neutrons - num_classified_neutrons)/num_neutrons)\n","  error_overall_gammas = 100*np.abs((num_gammas - num_classified_gammas)/num_gammas)\n","\n","  num_correct_neutrons = np.count_nonzero(np.logical_and(true_labels == 0, true_labels == method_labels))\n","  num_correct_gammas = np.count_nonzero(np.logical_and(true_labels == 1, true_labels == method_labels))\n","\n","  if num_neutrons == 0:\n","    neutron_acc = 100\n","  else:\n","    neutron_acc = 100*(num_correct_neutrons/num_neutrons)\n","\n","  if num_gammas == 0:\n","    gamma_acc = 100\n","  else:\n","    gamma_acc = 100*(num_correct_gammas/num_gammas)\n","\n","  return num_elements,num_neutrons,num_gammas, acc, neutron_acc, gamma_acc, num_classified_neutrons, num_classified_gammas, error_overall_neutrons, error_overall_gammas"],"metadata":{"id":"YYsRXv7Fb4d-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def split_data_for_new_bm(data,labels,N_single,N_double):\n","\n","  data_n1 = np.zeros((int(2*N_single),WL_classifier))\n","  data_n2 = np.zeros((int(4*N_double),WL_classifier))\n","  labels_n1 = np.zeros(int(2*N_single))\n","  labels_n2 = np.zeros(int(4*N_double))\n","\n","  j = 0\n","  k = 0\n","  for i in range(len(labels)):\n","    if labels[i] <= 1:\n","      data_n1[j,:] = data[i,:]\n","      labels_n1[j] = labels[i]\n","      j += 1\n","    else:\n","      data_n2[k,:] = data[i,:]\n","      labels_n2[k] = int(labels[i]-2)\n","      k += 1\n","\n","  return data_n1, labels_n1, data_n2, labels_n2\n","\n","def generate_discrete_data_BM_NN(data,arrival_times,arrival_labels,last_arrival_time, subtract_baseline = True, WL_classifier = 200):\n","\n","  coppied_arrival_times = np.copy(arrival_times)\n","  coppied_arrival_times = coppied_arrival_times.astype(int)\n","\n","  N_pulses = 0\n","  for i in range(len(coppied_arrival_times)):\n","    if coppied_arrival_times[i] >= cont_starting_time and coppied_arrival_times[i] < last_arrival_time and arrival_labels[i] != -1:\n","      N_pulses += 1\n","\n","  discrete_dataset = np.zeros((N_pulses,WL_classifier))\n","  discrete_labels = np.zeros(N_pulses)\n","  original_idx_lst = np.zeros(N_pulses)\n","\n","  j = 0\n","  i = 0\n","  while i < len(coppied_arrival_times) - 1:\n","    if coppied_arrival_times[i] >= cont_starting_time and coppied_arrival_times[i+1] < last_arrival_time and arrival_labels[i] != -1:\n","      if subtract_baseline == 'True':\n","        discrete_dataset[j,:] = data[coppied_arrival_times[i] - time_offset:coppied_arrival_times[i] - time_offset + WL_classifier] - data[coppied_arrival_times[i] - time_offset-1]\n","      else:\n","        discrete_dataset[j,:] = data[coppied_arrival_times[i] - time_offset:coppied_arrival_times[i] - time_offset + WL_classifier]\n","\n","      original_idx_lst[j] = i\n","\n","      if coppied_arrival_times[i+1] - coppied_arrival_times[i] > max_dist:\n","        discrete_labels[j] = arrival_labels[i]\n","        i += 1\n","      else:\n","        if arrival_labels[i] == 0 and arrival_labels[i+1] == 0:\n","          discrete_labels[j] = 2\n","        if arrival_labels[i] == 0 and arrival_labels[i+1] == 1:\n","          discrete_labels[j] = 3\n","        if arrival_labels[i] == 1 and arrival_labels[i+1] == 0:\n","          discrete_labels[j] = 4\n","        if arrival_labels[i] == 1 and arrival_labels[i+1] == 1:\n","          discrete_labels[j] = 5\n","        i += 2\n","      j += 1\n","    else:\n","      i += 1\n","\n","  return discrete_dataset, discrete_labels, original_idx_lst\n","\n","\n","def create_BM_FC_data_for_training(neutrons,gammas,N_single,N_double):\n","\n","  discrete_data_out = np.zeros((int(2*N_single + 4*N_double),WL_classifier))\n","  discrete_labels_out = np.zeros(int(2*N_single + 4*N_double))\n","  j = 0\n","  count_lst = np.zeros(6)\n","  count_thr_lst = np.array([N_single,N_single,N_double,N_double,N_double,N_double])\n","\n","  while(True):\n","    perm = np.random.permutation(np.shape(neutrons)[0])\n","    neutrons = neutrons[perm]\n","    gammas = gammas[perm]\n","    noised_data,labels,true_inf,last_arrival_time = generate_pulse_train(neutrons,gammas,lambda_n,\n","                                                                                          lambda_g, return_statistics = False, add_noise = True)\n","\n","    discrete_data, discrete_labels, original_idx_lst = generate_discrete_data_BM_NN(noised_data,true_inf[:,0],true_inf[:,1],last_arrival_time, subtract_baseline = True, WL_classifier = WL_classifier)\n","\n","    for i in range(len(discrete_labels)):\n","      if count_lst[int(discrete_labels[i])] < count_thr_lst[int(discrete_labels[i])]:\n","        discrete_data_out[j,:] = discrete_data[i,:]\n","        discrete_labels_out[j] = discrete_labels[i]\n","        count_lst[int(discrete_labels[i])] += 1\n","        j += 1\n","      if j == int(2*N_single + 4*N_double):\n","        perm = np.random.permutation(np.shape(discrete_data_out)[0])\n","        discrete_data_out = discrete_data_out[perm]\n","        discrete_labels_out = discrete_labels_out[perm]\n","\n","        return discrete_data_out, discrete_labels_out\n"],"metadata":{"id":"qA7LwiAIcegr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_final_Model_BM_NN12(data,true_labels,model1,params1,model2,params2):\n","\n","  method_labels = np.zeros(len(true_labels))\n","\n","  for i in range(len(true_labels)):\n","    if true_labels[i] <= 1:\n","      data1 = (data[i,:] - params1[0])/params1[1]\n","      data1 = torch.Tensor(data1)\n","      nn_values = model1(data1)\n","      method_labels[i] = torch.round(nn_values)\n","    else:\n","      data2 = (data[i,:] - params2[0])/params2[1]\n","      data2 = torch.Tensor(data2)\n","      nn_values = model2(data2)\n","      method_labels[i] = torch.argmax(nn_values) + 2\n","\n","  allc = 0\n","  corrc = 0\n","\n","  for i in range(len(true_labels)):\n","    if true_labels[i] <= 1:\n","      allc += 1\n","    else:\n","      allc += 2\n","    if true_labels[i] == method_labels[i]:\n","      if true_labels[i] <= 1:\n","        corrc += 1\n","      else:\n","        corrc += 2\n","    if true_labels[i] == 2 and (method_labels[i] == 3 or method_labels[i] == 4):\n","      corrc += 1\n","    if true_labels[i] == 5 and (method_labels[i] == 3 or method_labels[i] == 4):\n","      corrc += 1\n","    if true_labels[i] == 3 and (method_labels[i] == 2 or method_labels[i] == 5):\n","      corrc += 1\n","    if true_labels[i] == 4 and (method_labels[i] == 2 or method_labels[i] == 5):\n","      corrc += 1\n","\n","  if allc == 0:\n","    acc = 100\n","  else:\n","    acc = 100*(corrc/allc)\n","\n","  return acc\n","\n","def create_labels_Model_BM_NN(model,data,params):\n","\n","  data = (data - params[0])/params[1]\n","  data = torch.Tensor(data)\n","  nn_values = model(data)\n","  labels_out = torch.argmax(nn_values,axis=1)\n","\n","  return np.squeeze(labels_out.detach().numpy()), np.squeeze(nn_values.detach().numpy())\n","\n","def test_Model_BM_NN(data,arrival_times,arrival_labels,last_arrival_time, subtract_baseline = True, WL_classifier = 200):\n","\n","  coppied_arrival_times = np.copy(arrival_times)\n","  coppied_arrival_times = coppied_arrival_times.astype(int)\n","\n","  N_pulses = 0\n","  for i in range(len(coppied_arrival_times)):\n","    if coppied_arrival_times[i] >= cont_starting_time and coppied_arrival_times[i] < last_arrival_time and arrival_labels[i] != -1:\n","      N_pulses += 1\n","\n","  discrete_dataset = np.zeros((N_pulses,WL_classifier))\n","  discrete_labels = np.zeros(N_pulses)\n","  original_idx_lst = np.zeros(N_pulses)\n","\n","  j = 0\n","  i = 0\n","  while i < len(coppied_arrival_times) - 1:\n","    if coppied_arrival_times[i] >= cont_starting_time and coppied_arrival_times[i+1] < last_arrival_time and arrival_labels[i] != -1:\n","      if subtract_baseline == 'True':\n","        discrete_dataset[j,:] = data[coppied_arrival_times[i] - time_offset:coppied_arrival_times[i] - time_offset + WL_classifier] - data[coppied_arrival_times[i] - time_offset-1]\n","      else:\n","        discrete_dataset[j,:] = data[coppied_arrival_times[i] - time_offset:coppied_arrival_times[i] - time_offset + WL_classifier]\n","\n","      original_idx_lst[j] = i\n","\n","      if coppied_arrival_times[i+1] - coppied_arrival_times[i] > max_dist:\n","        discrete_labels[j] = arrival_labels[i]\n","        i += 1\n","      else:\n","        if arrival_labels[i] == 0 and arrival_labels[i+1] == 0:\n","          discrete_labels[j] = 2\n","        if arrival_labels[i] == 0 and arrival_labels[i+1] == 1:\n","          discrete_labels[j] = 3\n","        if arrival_labels[i] == 1 and arrival_labels[i+1] == 0:\n","          discrete_labels[j] = 4\n","        if arrival_labels[i] == 1 and arrival_labels[i+1] == 1:\n","          discrete_labels[j] = 5\n","        i += 2\n","      j += 1\n","    else:\n","      i += 1\n","\n","  return discrete_dataset, discrete_labels, original_idx_lst\n","\n","def test_final_Model_BM_NN(model,data,params,true_labels):\n","\n","  data = (data - params[0])/params[1]\n","  data = torch.Tensor(data)\n","  nn_values = model(data)\n","  method_labels = np.zeros(len(true_labels))\n","\n","  for i in range(len(true_labels)):\n","    if true_labels[i] <= 1:\n","      method_labels[i] = torch.argmax(nn_values[i,:2])\n","    else:\n","      method_labels[i] = torch.argmax(nn_values[i,2:]) + 2\n","\n","  allc = 0\n","  corrc = 0\n","\n","  for i in range(len(true_labels)):\n","    if true_labels[i] <= 1:\n","      allc += 1\n","    else:\n","      allc += 2\n","    if true_labels[i] == method_labels[i]:\n","      if true_labels[i] <= 1:\n","        corrc += 1\n","      else:\n","        corrc += 2\n","    if true_labels[i] == 2 and (method_labels[i] == 3 or method_labels[i] == 4):\n","      corrc += 1\n","    if true_labels[i] == 5 and (method_labels[i] == 3 or method_labels[i] == 4):\n","      corrc += 1\n","    if true_labels[i] == 3 and (method_labels[i] == 2 or method_labels[i] == 5):\n","      corrc += 1\n","    if true_labels[i] == 4 and (method_labels[i] == 2 or method_labels[i] == 5):\n","      corrc += 1\n","\n","  if allc == 0:\n","    acc = 100\n","  else:\n","    acc = 100*(corrc/allc)\n","\n","  return acc\n","\n","def Classifiers_statistics_results_BM_NN1(method_labels,true_labels):\n","\n","  allc = 0\n","  corrc = 0\n","\n","  for i in range(len(true_labels)):\n","    if true_labels[i] == 0:\n","      p1t = 0\n","      p2t = -1\n","    if true_labels[i] == 1:\n","      p1t = 1\n","      p2t = -1\n","    if true_labels[i] == 2:\n","      p1t = 0\n","      p2t = 0\n","    if true_labels[i] == 3:\n","      p1t = 1\n","      p2t = 0\n","    if true_labels[i] == 4:\n","      p1t = 0\n","      p2t = 1\n","    if true_labels[i] == 5:\n","      p1t = 1\n","      p2t = 1\n","\n","    if method_labels[i] == 0:\n","      p1p = 0\n","      p2p = -1\n","    if method_labels[i] == 1:\n","      p1p = 1\n","      p2p = -1\n","    if method_labels[i] == 2:\n","      p1p = 0\n","      p2p = 0\n","    if method_labels[i] == 3:\n","      p1p = 1\n","      p2p = 0\n","    if method_labels[i] == 4:\n","      p1p = 0\n","      p2p = 1\n","    if method_labels[i] == 5:\n","      p1p = 1\n","      p2p = 1\n","\n","\n","    if p2t == -1 and p2p == -1:\n","      if p1t == p1p:\n","        corrc += 1\n","      allc += 1\n","    if p2t != -1 and p2p != -1:\n","      if p1t == p1p:\n","        corrc += 1\n","      if p2t == p2p:\n","        corrc += 1\n","      allc += 2\n","    if p2t != -1 and p2p == -1:\n","      if p1p == p1t or p1p == p2t:\n","        corrc += 1\n","      allc += 2\n","    if p2t == -1 and p2p != -1:\n","      if p1p == p1t or p2p == p1t:\n","        corrc += 1\n","      allc += 1\n","\n","  if allc == 0:\n","    acc = 100\n","  else:\n","    acc = 100*(corrc/allc)\n","\n","  return acc\n","\n","def Classifiers_statistics_results_BM_NN2(method_labels,true_labels):\n","\n","  allc = 0\n","  corrc = 0\n","\n","  for i in range(len(true_labels)):\n","    if true_labels[i] == 0:\n","      p1t = 0\n","      p2t = -1\n","    if true_labels[i] == 1:\n","      p1t = 1\n","      p2t = -1\n","    if true_labels[i] == 2:\n","      p1t = 0\n","      p2t = 0\n","    if true_labels[i] == 3:\n","      p1t = 1\n","      p2t = 0\n","    if true_labels[i] == 4:\n","      p1t = 0\n","      p2t = 1\n","    if true_labels[i] == 5:\n","      p1t = 1\n","      p2t = 1\n","\n","    if method_labels[i] == 0:\n","      p1p = 0\n","      p2p = -1\n","    if method_labels[i] == 1:\n","      p1p = 1\n","      p2p = -1\n","    if method_labels[i] == 2:\n","      p1p = 0\n","      p2p = 0\n","    if method_labels[i] == 3:\n","      p1p = 1\n","      p2p = 0\n","    if method_labels[i] == 4:\n","      p1p = 0\n","      p2p = 1\n","    if method_labels[i] == 5:\n","      p1p = 1\n","      p2p = 1\n","\n","\n","    if p2t == -1 and p2p == -1:\n","      if p1t == p1p:\n","        corrc += 1\n","      allc += 1\n","    if p2t != -1 and p2p != -1:\n","      if p1t == p1p:\n","        corrc += 1\n","      if p2t == p2p:\n","        corrc += 1\n","      allc += 2\n","    if p2t != -1 and p2p == -1:\n","      if p1p == p1t or p1p == p2t:\n","        corrc += 1\n","      allc += 2\n","    if p2t == -1 and p2p != -1:\n","      if p1p == p1t or p2p == p1t:\n","        corrc += 1\n","      allc += 2\n","\n","  if allc == 0:\n","    acc = 100\n","  else:\n","    acc = 100*(corrc/allc)\n","\n","  return acc\n"],"metadata":{"id":"zqAzsE1FcfBe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Combined_Results_Function(result_trig_iters,neutrons,gammas):\n","\n","  statistics_mat = np.zeros(18)\n","\n","  for j in range(result_trig_iters):\n","    perm = np.random.permutation(np.shape(neutrons)[0])\n","    neutrons = neutrons[perm]\n","    gammas = gammas[perm]\n","    noised_data,labels,true_inf,last_arrival_time = generate_pulse_train(neutrons,gammas,lambda_n,\n","                                                                                          lambda_g, return_statistics = False, add_noise = True)\n","\n","    deep_trig_arrival_times = deep_trigger_arrival_times(noised_data, last_arrival_time, prob=True)\n","\n","    arrivals_out, triggers_out, triggered_events_out, false_triggers_array_out = match_pairs(true_inf[:,0],deep_trig_arrival_times,low_NN,high_NN)\n","\n","    triggered_labels = np.where(false_triggers_array_out == -1, -1, labels[false_triggers_array_out.astype(int)] - 1)\n","    true_labels = labels[arrivals_out.astype(int)] - 1\n","\n","    st_idx = max(arrivals_out[1],triggers_out[1])\n","    en_idx = min(arrivals_out[len(arrivals_out)-1],triggers_out[len(triggers_out)-1])\n","\n","    st1 = np.argmax(arrivals_out >= st_idx)\n","    st2 = np.argmax(triggers_out >= st_idx)\n","    en1 = len(arrivals_out) - np.argmax(arrivals_out[::-1] <= en_idx) - 1\n","    en2 = len(triggers_out) - np.argmax(triggers_out[::-1] <= en_idx) - 1\n","\n","    arrivals_out = arrivals_out[st1:en1]\n","    triggered_events_out = triggered_events_out[st1:en1]\n","    true_labels = true_labels[st1:en1]\n","    triggers_out = triggers_out[st2:en2]\n","    triggered_labels = triggered_labels[st2:en2]\n","\n","    discrete_data, discrete_labels, original_idx_lst,classification_features,attention_features = generate_discrete_data_with_FA(noised_data,triggers_out,triggered_labels,last_arrival_time, extra_features = True, rand_off = False, subtract_baseline = True, WL_classifier = WL_classifier)\n","\n","    attention_labels, attention_values = create_labels_Model_Attention(model_attention,discrete_data,classification_features,attention_features,norm_params_model_attention)\n","\n","    statistics_mat[:8] += calc_deep_trigger_window_statistics(arrivals_out,triggered_events_out,triggered_labels,true_labels)\n","    statistics_mat[8:] += Combined_Classifier_statistics_results(attention_labels,discrete_labels)\n","\n","  return statistics_mat/result_trig_iters"],"metadata":{"id":"J3ssSV1V3qcD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def divide_dist_to_n_g(method_values,original_labels):\n","\n","  method_values_n = []\n","  method_values_g = []\n","  for i in range(len(original_labels)):\n","    if original_labels[i] == 0:\n","      method_values_n.append(method_values[i])\n","    if original_labels[i] == 1:\n","      method_values_g.append(method_values[i])\n","\n","  return method_values_n, method_values_g"],"metadata":{"id":"LkEhFUYqi24W"},"execution_count":null,"outputs":[]}]}